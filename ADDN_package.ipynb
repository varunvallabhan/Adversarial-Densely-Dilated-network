{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADDN-package.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTdFdi8rEUnr",
        "colab_type": "code",
        "outputId": "6bb555aa-0a81-40c2-b569-d2f436d5f717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#connecting to the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzEIKA9MIDII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Headers and libraries\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import datetime\n",
        "from os import path, makedirs\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNswDOLxIQqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"\n",
        "    :dataset: string\n",
        "    :name_train: string\n",
        "    :name_mask: string\n",
        "    :val_img: string\n",
        "    :val_msk: string\n",
        "    :test: string\n",
        "    :rtype: numpy arrays (float32)\n",
        "    \n",
        "    This class deals with all the data related calls. The class is called by DataLoader(Path to the folder where you have the data,\n",
        "    name of the train file, name of the train mask, name of validation image file, name of the validation mask file, name of the test file)\n",
        "    All the files are numpy arrarys stored in .npy format\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset,name_train='',name_mask='',val_img='',val_msk='',test=''):\n",
        "        self.dataset = dataset\n",
        "        self.image=name_train\n",
        "        self.mask=name_mask\n",
        "        self.val_msk=val_msk\n",
        "        self.val_img=val_img\n",
        "        self.tst=test\n",
        "        \n",
        "        \n",
        "    def load_test(self):\n",
        "        return np.load(dataset+test)\n",
        "\n",
        "\n",
        "    def load_batch(self, batch_size=1):\n",
        "        imgs_trn = np.load(self.dataset+'/' + self.image)\n",
        "        msks_trn = np.load(self.dataset+'/' + self.mask)\n",
        "        self.n_batches = int(imgs_trn.shape[0] / batch_size)\n",
        "        \n",
        "        for i in range(self.n_batches - 1):\n",
        "            imgs =imgs_trn[i * batch_size:(i + 1) * batch_size]\n",
        "            labels = msks_trn[i * batch_size:(i + 1) * batch_size]\n",
        "            yield imgs, labels\n",
        "\n",
        "            \n",
        "    def load_img(self, batch_size=1):\n",
        "        imgs_val = np.load(self.dataset+'/'+ self.image)\n",
        "        msks_val = np.load(self.dataset +'/'+ self.mask)\n",
        "        batch_images = random.sample(range(imgs_val.shape[0]), batch_size)\n",
        "        return imgs_val[batch_images],msks_val[batch_images]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulKUaXeAIUKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "this is the dice coefficient loss which is used for training along side mse. \n",
        "EQN used: 2*sum(a.b)/sum(a)+sum(b) this is dice coef for our purpose.\n",
        "1-dice coeff is the loss.\n",
        "\n",
        "\"\"\"\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.0\n",
        "    y_true_flat = K.flatten(y_true)\n",
        "    y_pred_flat = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_flat * y_pred_flat)\n",
        "    return (2.0 * intersection + smooth) / (K.sum(y_true_flat) + K.sum(y_pred_flat) + smooth)\n",
        "\n",
        "  \n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - 1.0 * dice_coef(y_true, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIUbTordIWsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#network\n",
        "\"\"\"\n",
        "The get_densenet takes input shape and builds a dense dilated network. \n",
        "The feature size and dpeth are set to 4 which can be reduced in case of lower computational power.\n",
        "the build_discriminator function takes input size and discriminator factor as input the value DF is then used to determine the size of the layers.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def get_densenet(input_shape,depth,feature=48):\n",
        "    '''\n",
        "    :input_shape: (int,int,int)\n",
        "    :depth: int \n",
        "    :features: int (preferably 48)\n",
        "    :rtype: untrained model\n",
        "    \n",
        "    The feature size is the same as the one in the code base. Do not reduce the factor below 12 it results in poor feature extraction.\n",
        "    The depth can be adjusted according to need. Do not reduce below 2. you can go higher but would be computationally heavy.\n",
        "    \n",
        "    '''\n",
        "    K.set_image_dim_ordering('tf')\n",
        "    x = inputs = Input(shape=input_shape, dtype='float32')\n",
        "    x = Conv2D(feature, 3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = dc_0_out = dense_block(x,depth=depth)\n",
        "    x = transition_Down(x, feature*2)\n",
        "    x = dc_1_out = dense_block(x,depth=depth)\n",
        "    x = transition_Down(x, feature*3)\n",
        "    x = dc_2_out = dense_block(x,depth=depth)\n",
        "    x = transition_Down(x, feature*4)\n",
        "    x = dc_3_out = dense_block(x,depth=depth)\n",
        "    x = transition_Down(x, feature*5)\n",
        "    x = dense_block(x,depth=depth)\n",
        "    x = transition_Up(x, feature)\n",
        "    x = concatenate([x, dc_3_out])\n",
        "    x = dense_block(x,depth=depth)\n",
        "    x = transition_Up(x, feature)\n",
        "    x = concatenate([x, dc_2_out])\n",
        "    x = dense_block(x,depth=depth)\n",
        "    x = transition_Up(x, feature)\n",
        "    x = concatenate([x, dc_1_out])\n",
        "    x = dense_block(x,depth=depth)\n",
        "    x = transition_Up(x, feature)\n",
        "    x = concatenate([x, dc_0_out])\n",
        "    x = dense_block(x,depth=depth)\n",
        "    x = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    net = Model(inputs=inputs, outputs=x)\n",
        "    return net\n",
        "\n",
        "  \n",
        "def dense_block(input_layer, features=12, depth=4,temperature=1.0, padding='same', batchnorm=False,dropout=0.2):\n",
        "    '''\n",
        "    :input_layer: tensor object\n",
        "    :features: int\n",
        "    :depth: int \n",
        "    :temperature: float\n",
        "    :padding: string \n",
        "    :batchnorm: bool\n",
        "    :dropout: float\n",
        "    :rtype: a block of layers \n",
        "    \n",
        "    This is where we create the dense dilation blocks. \n",
        "    '''\n",
        "    inputs = x = input_layer\n",
        "    maps = [inputs]\n",
        "    dilation_rate = 1\n",
        "    kernel_size = (3, 3)\n",
        "    for n in range(depth):\n",
        "        x0 = x\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(features, kernel_size, dilation_rate=dilation_rate,\n",
        "                   padding=padding, kernel_initializer='he_normal')(x)\n",
        "        x = Dropout(dropout)(x)\n",
        "        maps.append(x)\n",
        "        if n!= depth-1:\n",
        "            x = Concatenate()([x0, x])\n",
        "        else:\n",
        "            x = Concatenate()(maps)\n",
        "        dilation_rate *= 2\n",
        "    return x\n",
        "\n",
        "\n",
        "def transition_Down(input_layer,features,kernel_size=(3,3), padding='same',dropout=0.2):\n",
        "    '''\n",
        "    :input_layer: tensor object\n",
        "    :features: int\n",
        "    :kernel_size: (int,int)\n",
        "    :padding: string\n",
        "    :dropout: float\n",
        "    :rtype: tensor object\n",
        "    \n",
        "    This is the downsampling. \n",
        "    '''\n",
        "    x = BatchNormalization()(input_layer)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(features, kernel_size,\n",
        "                   padding=padding, kernel_initializer='he_normal')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = MaxPooling2D(2,2)(x)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def transition_Up(input_layer, feature, kernel_size=(3,3),stride=2):\n",
        "    '''\n",
        "    :input_layer: tensor object\n",
        "    : features: int\n",
        "    :kernel_size: (int,int)\n",
        "    :stride: int\n",
        "    :rtype: tensor object\n",
        "    \n",
        "    This upsamples the feature block.\n",
        "    '''\n",
        "    x = Conv2DTranspose(feature, 2, strides=stride, activation='relu', kernel_initializer='he_normal')(input_layer)\n",
        "    return x\n",
        "\n",
        "  \n",
        "def build_discriminator(img_shape,df):\n",
        "    '''\n",
        "    :img_shape: (int,int,int)\n",
        "    :df: int\n",
        "    :rtype: untrained model\n",
        "    \n",
        "    This builds the discriminator network  with a custom feature size decided by df. \n",
        "    '''\n",
        "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "    def layer (layer_input, filters, f_size=4, bn=True):\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=1, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        return d\n",
        "    img_A = Input(shape=img_shape)\n",
        "    img_B = Input(shape=img_shape)\n",
        "\n",
        "    # Concatenate image and conditioning image by channels to produce input\n",
        "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
        "    d1 = d_layer(combined_imgs, df, bn=False)\n",
        "    d2 = d_layer(d1, df*2)\n",
        "    d3 = layer(d2, df*4)\n",
        "    # d4 = layer(d3, df*8)\n",
        "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d3)\n",
        "    model = Model([img_A, img_B], validity)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAHqzjUwIbPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main\n",
        "\n",
        "class ADDN():\n",
        "    \"\"\"\n",
        "    :checkpoint_name: string\n",
        "    :data: dict \n",
        "    \n",
        "    THe ADDN class is where training and testing is done. \n",
        "    Compile is where the model is initialised and compiled and train module trains the GAN.\n",
        "    Sample images is used to store imaged while training and saveResult is used to save predicted output during testing.\n",
        "    \"\"\"\n",
        "    def __init__(self,checkpoint_name,data):\n",
        "        ### Configurations\n",
        "        self.config = data\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        self.img_rows = int(self.config['input_shape'][0])\n",
        "        self.disc_patch = (self.config['patch'], self.config['patch'], 1)\n",
        "        self.data_loader = DataLoader(dataset=self.config['data_path'],name_train=self.config['train_image_name'],name_mask=self.config['train_mask_name'],test=self.config['test_image_name'])\n",
        "        self.checkpoint_name = checkpoint_name\n",
        "\n",
        "        self.generator = None\n",
        "        self.discriminator = None\n",
        "        self.combined = None\n",
        "        self.imgs_trn = None\n",
        "        self.msks_trn = None\n",
        "        self.imgs_val = None\n",
        "        self.msks_val = None\n",
        "        log_path = 'Graph/addn'\n",
        "        self.callback = TensorBoard(log_path)\n",
        "        return\n",
        "\n",
        "    \n",
        "    def checkpoint_path(self):\n",
        "        return self.config['data_path']+'%s' % (self.checkpoint_name)\n",
        "\n",
        "      \n",
        "    def compile(self):\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = build_discriminator(self.config['input_shape'], self.config['df'])\n",
        "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.discriminator.summary()\n",
        "        # Build the generator\n",
        "        self.generator = get_densenet(self.config['input_shape'],self.config['depth'],self.config['generator_factor'])\n",
        "        self.generator.summary()\n",
        "        #self.generator =get_generator(self.config['input_shape'])\n",
        "        img = Input(shape=self.config['input_shape'])\n",
        "        label = Input(shape=self.config['input_shape'])\n",
        "        seg = self.generator(img)\n",
        "        self.discriminator.trainable = False\n",
        "        valid = self.discriminator([seg, img])\n",
        "        self.combined = Model(inputs=[label, img], outputs=[valid, seg])\n",
        "        self.combined.compile(loss=['mse',dice_coef_loss], loss_weights=[1, 100], optimizer=optimizer)\n",
        "        #self.callback.set_model(self.generator)\n",
        "        return\n",
        "\n",
        "      \n",
        "    def train(self, sample=False):\n",
        "        start_time = datetime.datetime.now()\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((self.config['batch_size'],) + self.disc_patch)\n",
        "        fake = np.zeros((self.config['batch_size'],) + self.disc_patch)\n",
        "        gen_loss=[]\n",
        "        dis_loss=[]\n",
        "        acc=[]\n",
        "        for epoch in range(self.config['epochs']):\n",
        "            for batch_i, (imgs, labels) in enumerate(self.data_loader.load_batch(self.config['batch_size'])):\n",
        "                # Condition on B and generate a translated version\n",
        "\n",
        "                # Train the discriminators (original images = real / generated = Fake)\n",
        "                dl=100000\n",
        "                segs = self.generator.predict(imgs)\n",
        "                d_loss_real = self.discriminator.train_on_batch([labels, imgs], valid)\n",
        "                d_loss_fake = self.discriminator.train_on_batch([segs, imgs], fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "                # Train the generators\n",
        "                g_loss = self.combined.train_on_batch([labels, imgs], [valid, labels])\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "                # Plot the progress\n",
        "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, self.config['epochs'],\n",
        "                                                                        batch_i, self.data_loader.n_batches,\n",
        "                                                                        d_loss[0], 100*d_loss[1],\n",
        "                                                                        g_loss[0],\n",
        "                                                                        elapsed_time))\n",
        "                if dl>d_loss[0]:\n",
        "                  self.generator.save_weights(self.config['weights_path_with_name'])\n",
        "                  dl=d_loss[0]\n",
        "                  ac=acc\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                if sample == True:\n",
        "                    if batch_i % self.config['sample_interval'] == 0:\n",
        "                        self.sample_images(epoch, batch_i)\n",
        "            train_names = 'train_loss'\n",
        "            \n",
        "            gen_loss.append(g_loss)\n",
        "            dis_loss.append(d_loss)\n",
        "            acc.append(100*d_loss[1])\n",
        "        np.save(self.config['savepath']+'/discriminator.npy',dis_loss)\n",
        "        np.save(self.config['savepath']+'/generator.npy',gen_loss)\n",
        "        np.save(self.config['savepath']+'/accuracy.npy',acc)\n",
        "            \n",
        "        print(dl,ac)\n",
        "        return [gen_loss,dis_loss,acc]\n",
        "    \n",
        "    \n",
        "    def predict(self, imgs):\n",
        "        return self.generator.predict(imgs)\n",
        "\n",
        "      \n",
        "    def sample_images(self, epoch, batch_i):\n",
        "        # r, c = 3, 3\n",
        "        imgs, labels = self.data_loader.load_img(batch_size=1)\n",
        "        segs = self.predict(imgs)\n",
        "        print (segs.shape)\n",
        "        cv2.imwrite(self.config['save_train_images']+'/%d_%d.bmp' % (epoch, batch_i), segs[0][:, :, 0]*255)\n",
        "    \n",
        "    \n",
        "    def test(self):\n",
        "          imgtst=self.data_loader.load_test()\n",
        "          #print(imgtst.shape) \n",
        "\n",
        "          results=self.generator.predict(imgtst,batch_size=2, verbose=1)\n",
        "\n",
        "          def saveResult(self,npyfile):\n",
        "              for i in range(numpyfile.shape[0]):\n",
        "                  cv2.imwrite(self.config['save_path']+\"%d_predict_vm.png\"%i,npyfile[i]*255)\n",
        "          self.saveResult(self.config['savepath'],results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0_r7xkya6p8",
        "colab_type": "code",
        "outputId": "0693b57f-05c2-4eae-d1d1-3d83183c5a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        ":data:{\n",
        "        'data_path': path to the folder which has the data        \n",
        "        'train_image_name':name of the train image file\n",
        "        'train_mask_name':name of the train mask file\n",
        "        'val_image_name':name of the validation image file\n",
        "        'val_mask_name':name of the validation mask file\n",
        "        'test_image_name':name of the test file\n",
        "        \"weights_path_with_name\":'path to the loaction where you want to save the weights/nameof the file.hd5\n",
        "        'save_train_images':path to loaction where you want to save the images generated during training to check the progress\n",
        "        'savepath':path to loaction where you want to save the predicted output after training\n",
        "        'depth': depth of the dilated network\n",
        "        'input_shape': Input shape\n",
        "        'output_shape': output shape\n",
        "        'batch_size': batch size. If the model keep having memory error reduce the batch size. For 512x512 set to 1 if running on Colab.\n",
        "        'epochs': number of epochs\n",
        "        'sample_interval': frequency of posting images generated during training \n",
        "        'df':64, Discriminator factor. this decides the size of the layers of discriminator\n",
        "        'patch':this is  for stabalising the model. decided by imagesize/4. for 512-->128, 128-->32\n",
        "        'feature_factor': feature factor for generator. \n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "data={\n",
        "        'data_path': \"drive/My Drive/GAN_seg/ADDN-master/data\",\n",
        "        'save_train_images':'drive/My Drive/GAN_seg/ADDN-master/vm/train',\n",
        "        'train_image_name':'/new_img_trn.npy',\n",
        "        'train_mask_name':'/new_msk_trn.npy',\n",
        "        'val_image_name':\"\",\n",
        "        'val_mask_name':'',\n",
        "        'test_image_name':'/img_tst.npy',\n",
        "        \"weights_path_with_name\":'drive/My Drive/GAN_seg/ADDN-master/vm/checkpoints/pack.hd5',\n",
        "        'savepath':'drive/My Drive/GAN_seg/ADDN-master/vm/predict', \n",
        "        'depth':4,\n",
        "        'input_shape': (512, 512, 1),\n",
        "        'output_shape': (512, 512, 1),\n",
        "        'batch_size': 1,\n",
        "        'epochs': 5,\n",
        "        'sample_interval': 200,\n",
        "        'df':64,\n",
        "        'patch':128,\n",
        "        'generator_factor':48\n",
        "        }\n",
        "\n",
        "model=ADDN('trail',data)\n",
        "model.compile()\n",
        "gen,dis,acc= model.train(sample=True)\n",
        "model.test()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 512, 512, 2)  0           input_6[0][0]                    \n",
            "                                                                 input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 256, 256, 64) 2112        concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 256, 256, 64) 0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 128, 128, 128 131200      leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 128, 128, 128 0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 128, 128, 128 512         leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 128, 128, 256 524544      batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 128, 128, 256 0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 128, 128, 256 1024        leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 128, 128, 1)  4097        batch_normalization_44[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 663,489\n",
            "Trainable params: 662,721\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 512, 512, 48) 480         input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 512, 512, 48) 0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 512, 512, 48) 192         dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 512, 512, 48) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 512, 512, 12) 5196        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 512, 512, 12) 0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 512, 512, 60) 0           dropout_42[0][0]                 \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 512, 512, 60) 240         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 512, 512, 60) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 512, 512, 12) 6492        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 512, 512, 12) 0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 512, 512, 72) 0           concatenate_43[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 512, 512, 72) 288         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 512, 512, 72) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 512, 512, 12) 7788        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 512, 512, 12) 0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 512, 512, 84) 0           concatenate_44[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 512, 512, 84) 336         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 512, 512, 84) 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 512, 512, 12) 9084        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 512, 512, 12) 0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 512, 512, 96) 0           dropout_42[0][0]                 \n",
            "                                                                 dropout_43[0][0]                 \n",
            "                                                                 dropout_44[0][0]                 \n",
            "                                                                 dropout_45[0][0]                 \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 512, 512, 96) 384         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 512, 512, 96) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 512, 512, 96) 83040       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 512, 512, 96) 0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 96) 0           dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 256, 256, 96) 384         max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 256, 256, 96) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 256, 256, 12) 10380       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 256, 256, 12) 0           conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 256, 256, 108 0           max_pooling2d_5[0][0]            \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 256, 256, 108 432         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 256, 256, 108 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 256, 256, 12) 11676       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 256, 256, 12) 0           conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 256, 256, 120 0           concatenate_47[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 256, 256, 120 480         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 256, 256, 120 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 256, 256, 12) 12972       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 256, 256, 12) 0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 256, 256, 132 0           concatenate_48[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 256, 256, 132 528         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 256, 256, 132 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 256, 256, 12) 14268       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 256, 256, 12) 0           conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 256, 256, 144 0           max_pooling2d_5[0][0]            \n",
            "                                                                 dropout_48[0][0]                 \n",
            "                                                                 dropout_49[0][0]                 \n",
            "                                                                 dropout_50[0][0]                 \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 256, 256, 144 576         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 256, 256, 144 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 256, 256, 144 186768      activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 256, 256, 144 0           conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 144 0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 128, 128, 144 576         max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 128, 128, 144 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 128, 128, 12) 15564       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 128, 128, 12) 0           conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, 128, 128, 156 0           max_pooling2d_6[0][0]            \n",
            "                                                                 dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 128, 128, 156 624         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 128, 128, 156 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 128, 128, 12) 16860       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 128, 128, 12) 0           conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, 128, 128, 168 0           concatenate_51[0][0]             \n",
            "                                                                 dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 128, 128, 168 672         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 128, 128, 168 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 128, 128, 12) 18156       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 128, 128, 12) 0           conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, 128, 128, 180 0           concatenate_52[0][0]             \n",
            "                                                                 dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 128, 128, 180 720         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 128, 128, 180 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 128, 128, 12) 19452       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 128, 128, 12) 0           conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, 128, 128, 192 0           max_pooling2d_6[0][0]            \n",
            "                                                                 dropout_53[0][0]                 \n",
            "                                                                 dropout_54[0][0]                 \n",
            "                                                                 dropout_55[0][0]                 \n",
            "                                                                 dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 128, 128, 192 768         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 128, 128, 192 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 128, 128, 192 331968      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 128, 128, 192 0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 192)  0           dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 64, 64, 192)  768         max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 64, 64, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 64, 64, 12)   20748       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 64, 64, 12)   0           conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 64, 64, 204)  0           max_pooling2d_7[0][0]            \n",
            "                                                                 dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 64, 64, 204)  816         concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 64, 64, 204)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 64, 64, 12)   22044       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 64, 64, 12)   0           conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 64, 64, 216)  0           concatenate_55[0][0]             \n",
            "                                                                 dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 64, 64, 216)  864         concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 64, 64, 216)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 64, 64, 12)   23340       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 64, 64, 12)   0           conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 64, 64, 228)  0           concatenate_56[0][0]             \n",
            "                                                                 dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 64, 64, 228)  912         concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 64, 64, 228)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 64, 64, 12)   24636       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 64, 64, 12)   0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 64, 64, 240)  0           max_pooling2d_7[0][0]            \n",
            "                                                                 dropout_58[0][0]                 \n",
            "                                                                 dropout_59[0][0]                 \n",
            "                                                                 dropout_60[0][0]                 \n",
            "                                                                 dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 64, 64, 240)  960         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 64, 64, 240)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 64, 64, 240)  518640      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 64, 64, 240)  0           conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 240)  0           dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 240)  960         max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 240)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 12)   25932       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 32, 32, 12)   0           conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 32, 32, 252)  0           max_pooling2d_8[0][0]            \n",
            "                                                                 dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 32, 32, 252)  1008        concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 252)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 32, 32, 12)   27228       activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 32, 32, 12)   0           conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 32, 32, 264)  0           concatenate_59[0][0]             \n",
            "                                                                 dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 32, 32, 264)  1056        concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 264)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 32, 32, 12)   28524       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 32, 32, 12)   0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 32, 32, 276)  0           concatenate_60[0][0]             \n",
            "                                                                 dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 32, 32, 276)  1104        concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 276)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 32, 32, 12)   29820       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 32, 32, 12)   0           conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 32, 32, 288)  0           max_pooling2d_8[0][0]            \n",
            "                                                                 dropout_63[0][0]                 \n",
            "                                                                 dropout_64[0][0]                 \n",
            "                                                                 dropout_65[0][0]                 \n",
            "                                                                 dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 48)   55344       concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 64, 64, 288)  0           conv2d_transpose_5[0][0]         \n",
            "                                                                 concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 64, 64, 288)  1152        concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 64, 64, 288)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 64, 64, 12)   31116       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 64, 64, 12)   0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 64, 64, 300)  0           concatenate_63[0][0]             \n",
            "                                                                 dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 64, 64, 300)  1200        concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 64, 64, 300)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 64, 64, 12)   32412       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 64, 64, 12)   0           conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 64, 64, 312)  0           concatenate_64[0][0]             \n",
            "                                                                 dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 64, 64, 312)  1248        concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 64, 64, 312)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 64, 64, 12)   33708       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 64, 64, 12)   0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 64, 64, 324)  0           concatenate_65[0][0]             \n",
            "                                                                 dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 64, 64, 324)  1296        concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 64, 64, 324)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 64, 64, 12)   35004       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_70 (Dropout)            (None, 64, 64, 12)   0           conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, 64, 64, 336)  0           concatenate_63[0][0]             \n",
            "                                                                 dropout_67[0][0]                 \n",
            "                                                                 dropout_68[0][0]                 \n",
            "                                                                 dropout_69[0][0]                 \n",
            "                                                                 dropout_70[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 48) 64560       concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, 128, 128, 240 0           conv2d_transpose_6[0][0]         \n",
            "                                                                 concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 128, 128, 240 960         concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 128, 128, 240 0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 128, 128, 12) 25932       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_71 (Dropout)            (None, 128, 128, 12) 0           conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_69 (Concatenate)    (None, 128, 128, 252 0           concatenate_68[0][0]             \n",
            "                                                                 dropout_71[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 128, 128, 252 1008        concatenate_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 128, 128, 252 0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 128, 128, 12) 27228       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_72 (Dropout)            (None, 128, 128, 12) 0           conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_70 (Concatenate)    (None, 128, 128, 264 0           concatenate_69[0][0]             \n",
            "                                                                 dropout_72[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 128, 128, 264 1056        concatenate_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 128, 128, 264 0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 128, 128, 12) 28524       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_73 (Dropout)            (None, 128, 128, 12) 0           conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_71 (Concatenate)    (None, 128, 128, 276 0           concatenate_70[0][0]             \n",
            "                                                                 dropout_73[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 128, 128, 276 1104        concatenate_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 128, 128, 276 0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 128, 128, 12) 29820       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_74 (Dropout)            (None, 128, 128, 12) 0           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_72 (Concatenate)    (None, 128, 128, 288 0           concatenate_68[0][0]             \n",
            "                                                                 dropout_71[0][0]                 \n",
            "                                                                 dropout_72[0][0]                 \n",
            "                                                                 dropout_73[0][0]                 \n",
            "                                                                 dropout_74[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 48) 55344       concatenate_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_73 (Concatenate)    (None, 256, 256, 192 0           conv2d_transpose_7[0][0]         \n",
            "                                                                 concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 256, 256, 192 768         concatenate_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 256, 256, 192 0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 256, 256, 12) 20748       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 256, 256, 12) 0           conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_74 (Concatenate)    (None, 256, 256, 204 0           concatenate_73[0][0]             \n",
            "                                                                 dropout_75[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 256, 256, 204 816         concatenate_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 256, 256, 204 0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 256, 256, 12) 22044       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_76 (Dropout)            (None, 256, 256, 12) 0           conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_75 (Concatenate)    (None, 256, 256, 216 0           concatenate_74[0][0]             \n",
            "                                                                 dropout_76[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 256, 256, 216 864         concatenate_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 256, 256, 216 0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 256, 256, 12) 23340       activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 256, 256, 12) 0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_76 (Concatenate)    (None, 256, 256, 228 0           concatenate_75[0][0]             \n",
            "                                                                 dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 256, 256, 228 912         concatenate_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 256, 256, 228 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 256, 256, 12) 24636       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 256, 256, 12) 0           conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_77 (Concatenate)    (None, 256, 256, 240 0           concatenate_73[0][0]             \n",
            "                                                                 dropout_75[0][0]                 \n",
            "                                                                 dropout_76[0][0]                 \n",
            "                                                                 dropout_77[0][0]                 \n",
            "                                                                 dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTrans (None, 512, 512, 48) 46128       concatenate_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_78 (Concatenate)    (None, 512, 512, 144 0           conv2d_transpose_8[0][0]         \n",
            "                                                                 concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 512, 512, 144 576         concatenate_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 512, 512, 144 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 512, 512, 12) 15564       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 512, 512, 12) 0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_79 (Concatenate)    (None, 512, 512, 156 0           concatenate_78[0][0]             \n",
            "                                                                 dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 512, 512, 156 624         concatenate_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 512, 512, 156 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 512, 512, 12) 16860       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 512, 512, 12) 0           conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_80 (Concatenate)    (None, 512, 512, 168 0           concatenate_79[0][0]             \n",
            "                                                                 dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 512, 512, 168 672         concatenate_80[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 512, 512, 168 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 512, 512, 12) 18156       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 512, 512, 12) 0           conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_81 (Concatenate)    (None, 512, 512, 180 0           concatenate_80[0][0]             \n",
            "                                                                 dropout_81[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 512, 512, 180 720         concatenate_81[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 512, 512, 180 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 512, 512, 12) 19452       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 512, 512, 12) 0           conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_82 (Concatenate)    (None, 512, 512, 192 0           concatenate_78[0][0]             \n",
            "                                                                 dropout_79[0][0]                 \n",
            "                                                                 dropout_80[0][0]                 \n",
            "                                                                 dropout_81[0][0]                 \n",
            "                                                                 dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 512, 512, 1)  193         concatenate_82[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 2,127,793\n",
            "Trainable params: 2,112,481\n",
            "Non-trainable params: 15,312\n",
            "__________________________________________________________________________________________________\n",
            "[Epoch 0/5] [Batch 0/2400] [D loss: 2.394516, acc:  25%] [G loss: 32.215412] time: 0:00:37.866699\n",
            "(1, 512, 512, 1)\n",
            "[Epoch 0/5] [Batch 1/2400] [D loss: 5.376678, acc:  15%] [G loss: 23.404821] time: 0:01:38.828433\n",
            "[Epoch 0/5] [Batch 2/2400] [D loss: 3.017947, acc:  21%] [G loss: 22.848101] time: 0:01:40.028743\n",
            "[Epoch 0/5] [Batch 3/2400] [D loss: 1.743855, acc:  37%] [G loss: 21.809397] time: 0:01:41.226937\n",
            "[Epoch 0/5] [Batch 4/2400] [D loss: 2.012705, acc:  26%] [G loss: 14.937667] time: 0:01:42.422622\n",
            "[Epoch 0/5] [Batch 5/2400] [D loss: 1.328043, acc:  38%] [G loss: 14.636602] time: 0:01:43.630586\n",
            "[Epoch 0/5] [Batch 6/2400] [D loss: 1.014652, acc:  38%] [G loss: 20.057493] time: 0:01:44.830692\n",
            "[Epoch 0/5] [Batch 7/2400] [D loss: 0.918995, acc:  43%] [G loss: 19.420757] time: 0:01:46.049005\n",
            "[Epoch 0/5] [Batch 8/2400] [D loss: 0.934235, acc:  37%] [G loss: 14.136360] time: 0:01:47.250422\n",
            "[Epoch 0/5] [Batch 9/2400] [D loss: 0.877838, acc:  40%] [G loss: 19.641417] time: 0:01:48.463906\n",
            "[Epoch 0/5] [Batch 10/2400] [D loss: 0.868997, acc:  34%] [G loss: 14.015474] time: 0:01:49.664223\n",
            "[Epoch 0/5] [Batch 11/2400] [D loss: 0.741687, acc:  39%] [G loss: 13.964808] time: 0:01:50.898553\n",
            "[Epoch 0/5] [Batch 12/2400] [D loss: 0.673437, acc:  42%] [G loss: 13.914277] time: 0:01:52.130660\n",
            "[Epoch 0/5] [Batch 13/2400] [D loss: 0.647638, acc:  44%] [G loss: 13.874490] time: 0:01:53.367915\n",
            "[Epoch 0/5] [Batch 14/2400] [D loss: 0.627955, acc:  45%] [G loss: 13.840531] time: 0:01:54.602654\n",
            "[Epoch 0/5] [Batch 15/2400] [D loss: 0.618086, acc:  45%] [G loss: 13.818347] time: 0:01:55.845573\n",
            "[Epoch 0/5] [Batch 16/2400] [D loss: 0.601998, acc:  45%] [G loss: 13.775973] time: 0:01:57.097198\n",
            "[Epoch 0/5] [Batch 17/2400] [D loss: 0.596097, acc:  45%] [G loss: 13.758112] time: 0:01:58.341243\n",
            "[Epoch 0/5] [Batch 18/2400] [D loss: 0.590381, acc:  45%] [G loss: 18.862490] time: 0:01:59.596789\n",
            "[Epoch 0/5] [Batch 19/2400] [D loss: 0.613111, acc:  43%] [G loss: 13.730222] time: 0:02:00.845600\n",
            "[Epoch 0/5] [Batch 20/2400] [D loss: 0.654701, acc:  38%] [G loss: 13.747416] time: 0:02:02.102283\n",
            "[Epoch 0/5] [Batch 21/2400] [D loss: 0.674445, acc:  36%] [G loss: 13.778606] time: 0:02:03.348132\n",
            "[Epoch 0/5] [Batch 22/2400] [D loss: 0.677744, acc:  52%] [G loss: 13.742520] time: 0:02:04.592189\n",
            "[Epoch 0/5] [Batch 23/2400] [D loss: 1.481906, acc:  29%] [G loss: 13.799348] time: 0:02:05.844290\n",
            "[Epoch 0/5] [Batch 24/2400] [D loss: 2.729101, acc:  31%] [G loss: 13.957525] time: 0:02:07.093207\n",
            "[Epoch 0/5] [Batch 25/2400] [D loss: 2.028760, acc:  20%] [G loss: 13.717129] time: 0:02:08.334308\n",
            "[Epoch 0/5] [Batch 26/2400] [D loss: 0.852184, acc:  37%] [G loss: 13.706804] time: 0:02:09.572732\n",
            "[Epoch 0/5] [Batch 27/2400] [D loss: 0.638486, acc:  48%] [G loss: 13.623748] time: 0:02:10.821582\n",
            "[Epoch 0/5] [Batch 28/2400] [D loss: 0.603188, acc:  50%] [G loss: 13.648001] time: 0:02:12.073713\n",
            "[Epoch 0/5] [Batch 29/2400] [D loss: 0.583799, acc:  46%] [G loss: 13.591171] time: 0:02:13.322816\n",
            "[Epoch 0/5] [Batch 30/2400] [D loss: 0.534983, acc:  49%] [G loss: 13.607894] time: 0:02:14.568828\n",
            "[Epoch 0/5] [Batch 31/2400] [D loss: 0.517036, acc:  49%] [G loss: 13.568713] time: 0:02:15.821232\n",
            "[Epoch 0/5] [Batch 32/2400] [D loss: 0.501789, acc:  49%] [G loss: 13.579185] time: 0:02:17.077832\n",
            "[Epoch 0/5] [Batch 33/2400] [D loss: 0.498730, acc:  48%] [G loss: 13.541634] time: 0:02:18.324234\n",
            "[Epoch 0/5] [Batch 34/2400] [D loss: 0.500363, acc:  49%] [G loss: 19.595175] time: 0:02:19.565814\n",
            "[Epoch 0/5] [Batch 35/2400] [D loss: 0.527360, acc:  47%] [G loss: 19.452387] time: 0:02:20.804946\n",
            "[Epoch 0/5] [Batch 36/2400] [D loss: 0.540179, acc:  48%] [G loss: 13.564775] time: 0:02:22.047205\n",
            "[Epoch 0/5] [Batch 37/2400] [D loss: 0.499500, acc:  49%] [G loss: 20.118467] time: 0:02:23.316445\n",
            "[Epoch 0/5] [Batch 38/2400] [D loss: 0.507079, acc:  50%] [G loss: 13.551695] time: 0:02:24.571195\n",
            "[Epoch 0/5] [Batch 39/2400] [D loss: 0.454839, acc:  51%] [G loss: 19.226337] time: 0:02:25.824289\n",
            "[Epoch 0/5] [Batch 40/2400] [D loss: 0.453897, acc:  51%] [G loss: 13.530263] time: 0:02:27.078158\n",
            "[Epoch 0/5] [Batch 41/2400] [D loss: 0.425706, acc:  52%] [G loss: 19.575546] time: 0:02:28.320439\n",
            "[Epoch 0/5] [Batch 42/2400] [D loss: 0.425101, acc:  50%] [G loss: 13.509491] time: 0:02:29.569490\n",
            "[Epoch 0/5] [Batch 43/2400] [D loss: 0.418582, acc:  50%] [G loss: 13.490688] time: 0:02:30.831602\n",
            "[Epoch 0/5] [Batch 44/2400] [D loss: 0.412259, acc:  49%] [G loss: 13.497686] time: 0:02:32.090161\n",
            "[Epoch 0/5] [Batch 45/2400] [D loss: 0.407349, acc:  50%] [G loss: 13.480536] time: 0:02:33.340020\n",
            "[Epoch 0/5] [Batch 46/2400] [D loss: 0.429503, acc:  50%] [G loss: 19.281759] time: 0:02:34.598526\n",
            "[Epoch 0/5] [Batch 47/2400] [D loss: 0.491058, acc:  44%] [G loss: 13.472174] time: 0:02:35.850682\n",
            "[Epoch 0/5] [Batch 48/2400] [D loss: 0.459620, acc:  49%] [G loss: 13.502375] time: 0:02:37.105778\n",
            "[Epoch 0/5] [Batch 49/2400] [D loss: 0.443368, acc:  49%] [G loss: 13.464769] time: 0:02:38.357925\n",
            "[Epoch 0/5] [Batch 50/2400] [D loss: 0.484812, acc:  52%] [G loss: 19.565268] time: 0:02:39.602682\n",
            "[Epoch 0/5] [Batch 51/2400] [D loss: 0.576952, acc:  41%] [G loss: 13.478323] time: 0:02:40.856301\n",
            "[Epoch 0/5] [Batch 52/2400] [D loss: 0.495750, acc:  50%] [G loss: 13.507180] time: 0:02:42.114707\n",
            "[Epoch 0/5] [Batch 53/2400] [D loss: 0.468656, acc:  48%] [G loss: 13.461268] time: 0:02:43.354633\n",
            "[Epoch 0/5] [Batch 54/2400] [D loss: 0.480533, acc:  54%] [G loss: 18.947212] time: 0:02:44.607457\n",
            "[Epoch 0/5] [Batch 55/2400] [D loss: 0.510201, acc:  44%] [G loss: 13.458741] time: 0:02:45.936813\n",
            "[Epoch 0/5] [Batch 56/2400] [D loss: 0.489206, acc:  51%] [G loss: 19.187304] time: 0:02:47.178964\n",
            "[Epoch 0/5] [Batch 57/2400] [D loss: 0.518163, acc:  42%] [G loss: 13.466212] time: 0:02:48.427023\n",
            "[Epoch 0/5] [Batch 58/2400] [D loss: 0.419237, acc:  51%] [G loss: 13.476590] time: 0:02:49.672144\n",
            "[Epoch 0/5] [Batch 59/2400] [D loss: 0.380346, acc:  54%] [G loss: 18.487061] time: 0:02:50.923961\n",
            "[Epoch 0/5] [Batch 60/2400] [D loss: 0.375859, acc:  53%] [G loss: 13.464347] time: 0:02:52.170553\n",
            "[Epoch 0/5] [Batch 61/2400] [D loss: 0.369980, acc:  52%] [G loss: 13.446141] time: 0:02:53.423893\n",
            "[Epoch 0/5] [Batch 62/2400] [D loss: 0.354437, acc:  54%] [G loss: 13.459992] time: 0:02:54.665685\n",
            "[Epoch 0/5] [Batch 63/2400] [D loss: 0.345326, acc:  52%] [G loss: 13.438131] time: 0:02:55.917134\n",
            "[Epoch 0/5] [Batch 64/2400] [D loss: 0.342027, acc:  52%] [G loss: 13.452397] time: 0:02:57.176807\n",
            "[Epoch 0/5] [Batch 65/2400] [D loss: 0.333955, acc:  53%] [G loss: 19.550299] time: 0:02:58.438024\n",
            "[Epoch 0/5] [Batch 66/2400] [D loss: 0.326217, acc:  53%] [G loss: 13.445272] time: 0:02:59.686067\n",
            "[Epoch 0/5] [Batch 67/2400] [D loss: 0.319865, acc:  54%] [G loss: 19.914192] time: 0:03:00.961780\n",
            "[Epoch 0/5] [Batch 68/2400] [D loss: 0.341473, acc:  50%] [G loss: 13.440313] time: 0:03:02.236654\n",
            "[Epoch 0/5] [Batch 69/2400] [D loss: 0.334380, acc:  51%] [G loss: 18.420607] time: 0:03:03.504243\n",
            "[Epoch 0/5] [Batch 70/2400] [D loss: 0.340371, acc:  51%] [G loss: 13.448928] time: 0:03:04.772068\n",
            "[Epoch 0/5] [Batch 71/2400] [D loss: 0.324195, acc:  55%] [G loss: 13.444880] time: 0:03:06.019100\n",
            "[Epoch 0/5] [Batch 72/2400] [D loss: 0.357982, acc:  56%] [G loss: 19.812876] time: 0:03:07.247448\n",
            "[Epoch 0/5] [Batch 73/2400] [D loss: 0.490625, acc:  40%] [G loss: 13.452604] time: 0:03:08.486950\n",
            "[Epoch 0/5] [Batch 74/2400] [D loss: 0.417316, acc:  47%] [G loss: 13.456344] time: 0:03:09.720035\n",
            "[Epoch 0/5] [Batch 75/2400] [D loss: 0.342703, acc:  61%] [G loss: 13.457899] time: 0:03:10.963326\n",
            "[Epoch 0/5] [Batch 76/2400] [D loss: 0.363884, acc:  52%] [G loss: 13.435987] time: 0:03:12.213066\n",
            "[Epoch 0/5] [Batch 77/2400] [D loss: 0.370640, acc:  55%] [G loss: 13.453986] time: 0:03:13.475328\n",
            "[Epoch 0/5] [Batch 78/2400] [D loss: 0.350141, acc:  47%] [G loss: 13.444500] time: 0:03:14.735710\n",
            "[Epoch 0/5] [Batch 79/2400] [D loss: 0.311535, acc:  54%] [G loss: 13.448709] time: 0:03:16.001751\n",
            "[Epoch 0/5] [Batch 80/2400] [D loss: 0.294745, acc:  59%] [G loss: 13.446692] time: 0:03:17.260188\n",
            "[Epoch 0/5] [Batch 81/2400] [D loss: 0.291909, acc:  57%] [G loss: 13.444611] time: 0:03:18.533496\n",
            "[Epoch 0/5] [Batch 82/2400] [D loss: 0.293499, acc:  56%] [G loss: 13.443549] time: 0:03:19.797162\n",
            "[Epoch 0/5] [Batch 83/2400] [D loss: 0.282594, acc:  57%] [G loss: 13.448412] time: 0:03:21.064112\n",
            "[Epoch 0/5] [Batch 84/2400] [D loss: 0.278265, acc:  57%] [G loss: 13.437082] time: 0:03:22.321014\n",
            "[Epoch 0/5] [Batch 85/2400] [D loss: 0.280167, acc:  61%] [G loss: 13.451705] time: 0:03:23.588748\n",
            "[Epoch 0/5] [Batch 86/2400] [D loss: 0.298714, acc:  53%] [G loss: 13.438259] time: 0:03:24.842323\n",
            "[Epoch 0/5] [Batch 87/2400] [D loss: 0.291313, acc:  56%] [G loss: 13.454813] time: 0:03:26.105911\n",
            "[Epoch 0/5] [Batch 88/2400] [D loss: 0.275885, acc:  60%] [G loss: 13.449803] time: 0:03:27.382645\n",
            "[Epoch 0/5] [Batch 89/2400] [D loss: 0.282793, acc:  58%] [G loss: 13.443798] time: 0:03:28.651136\n",
            "[Epoch 0/5] [Batch 90/2400] [D loss: 0.325441, acc:  60%] [G loss: 13.461784] time: 0:03:29.917825\n",
            "[Epoch 0/5] [Batch 91/2400] [D loss: 0.366439, acc:  46%] [G loss: 13.466955] time: 0:03:31.179545\n",
            "[Epoch 0/5] [Batch 92/2400] [D loss: 0.329056, acc:  55%] [G loss: 13.459993] time: 0:03:32.535118\n",
            "[Epoch 0/5] [Batch 93/2400] [D loss: 0.308223, acc:  68%] [G loss: 13.475302] time: 0:03:33.805103\n",
            "[Epoch 0/5] [Batch 94/2400] [D loss: 0.455052, acc:  46%] [G loss: 13.439962] time: 0:03:35.067021\n",
            "[Epoch 0/5] [Batch 95/2400] [D loss: 0.569915, acc:  52%] [G loss: 13.500802] time: 0:03:36.308786\n",
            "[Epoch 0/5] [Batch 96/2400] [D loss: 0.505836, acc:  40%] [G loss: 19.476755] time: 0:03:37.569101\n",
            "[Epoch 0/5] [Batch 97/2400] [D loss: 0.372862, acc:  55%] [G loss: 13.487338] time: 0:03:38.840981\n",
            "[Epoch 0/5] [Batch 98/2400] [D loss: 0.298619, acc:  65%] [G loss: 13.464644] time: 0:03:40.117063\n",
            "[Epoch 0/5] [Batch 99/2400] [D loss: 0.283208, acc:  64%] [G loss: 18.904842] time: 0:03:41.381162\n",
            "[Epoch 0/5] [Batch 100/2400] [D loss: 0.375854, acc:  48%] [G loss: 14.659104] time: 0:03:42.642612\n",
            "[Epoch 0/5] [Batch 101/2400] [D loss: 0.474605, acc:  58%] [G loss: 14.721444] time: 0:03:43.921554\n",
            "[Epoch 0/5] [Batch 102/2400] [D loss: 0.583254, acc:  37%] [G loss: 14.670873] time: 0:03:45.179384\n",
            "[Epoch 0/5] [Batch 103/2400] [D loss: 0.616024, acc:  54%] [G loss: 14.753135] time: 0:03:46.434576\n",
            "[Epoch 0/5] [Batch 104/2400] [D loss: 0.721210, acc:  38%] [G loss: 20.740665] time: 0:03:47.694310\n",
            "[Epoch 0/5] [Batch 105/2400] [D loss: 1.042052, acc:  48%] [G loss: 14.857605] time: 0:03:48.964148\n",
            "[Epoch 0/5] [Batch 106/2400] [D loss: 1.472189, acc:  28%] [G loss: 21.360527] time: 0:03:50.225472\n",
            "[Epoch 0/5] [Batch 107/2400] [D loss: 2.031799, acc:  26%] [G loss: 15.051295] time: 0:03:51.490347\n",
            "[Epoch 0/5] [Batch 108/2400] [D loss: 2.182563, acc:  25%] [G loss: 14.863719] time: 0:03:52.754288\n",
            "[Epoch 0/5] [Batch 109/2400] [D loss: 1.463881, acc:  38%] [G loss: 14.912990] time: 0:03:54.039145\n",
            "[Epoch 0/5] [Batch 110/2400] [D loss: 0.844487, acc:  46%] [G loss: 14.794029] time: 0:03:55.303068\n",
            "[Epoch 0/5] [Batch 111/2400] [D loss: 0.573563, acc:  68%] [G loss: 21.200600] time: 0:03:56.640327\n",
            "[Epoch 0/5] [Batch 112/2400] [D loss: 0.407365, acc:  62%] [G loss: 21.122122] time: 0:03:57.907690\n",
            "[Epoch 0/5] [Batch 113/2400] [D loss: 0.275936, acc:  73%] [G loss: 20.396084] time: 0:03:59.167330\n",
            "[Epoch 0/5] [Batch 114/2400] [D loss: 0.264048, acc:  62%] [G loss: 14.721614] time: 0:04:00.444676\n",
            "[Epoch 0/5] [Batch 115/2400] [D loss: 0.235478, acc:  67%] [G loss: 14.727438] time: 0:04:01.709079\n",
            "[Epoch 0/5] [Batch 116/2400] [D loss: 0.228260, acc:  63%] [G loss: 14.720356] time: 0:04:02.972199\n",
            "[Epoch 0/5] [Batch 117/2400] [D loss: 0.221164, acc:  67%] [G loss: 20.341286] time: 0:04:04.231706\n",
            "[Epoch 0/5] [Batch 118/2400] [D loss: 0.232963, acc:  63%] [G loss: 14.710009] time: 0:04:05.507614\n",
            "[Epoch 0/5] [Batch 119/2400] [D loss: 0.218936, acc:  69%] [G loss: 14.715963] time: 0:04:06.775763\n",
            "[Epoch 0/5] [Batch 120/2400] [D loss: 0.211116, acc:  66%] [G loss: 20.362581] time: 0:04:08.041151\n",
            "[Epoch 0/5] [Batch 121/2400] [D loss: 0.227292, acc:  63%] [G loss: 14.707360] time: 0:04:09.301465\n",
            "[Epoch 0/5] [Batch 122/2400] [D loss: 0.212483, acc:  70%] [G loss: 14.707446] time: 0:04:10.543141\n",
            "[Epoch 0/5] [Batch 123/2400] [D loss: 0.205678, acc:  66%] [G loss: 14.702692] time: 0:04:11.789942\n",
            "[Epoch 0/5] [Batch 124/2400] [D loss: 0.202949, acc:  69%] [G loss: 14.705703] time: 0:04:13.074837\n",
            "[Epoch 0/5] [Batch 125/2400] [D loss: 0.199680, acc:  68%] [G loss: 14.702596] time: 0:04:14.323463\n",
            "[Epoch 0/5] [Batch 126/2400] [D loss: 0.198154, acc:  69%] [G loss: 14.702520] time: 0:04:15.585495\n",
            "[Epoch 0/5] [Batch 127/2400] [D loss: 0.195623, acc:  69%] [G loss: 14.695449] time: 0:04:16.848277\n",
            "[Epoch 0/5] [Batch 128/2400] [D loss: 0.193125, acc:  71%] [G loss: 14.701996] time: 0:04:18.112823\n",
            "[Epoch 0/5] [Batch 129/2400] [D loss: 0.186990, acc:  73%] [G loss: 20.688145] time: 0:04:19.392207\n",
            "[Epoch 0/5] [Batch 130/2400] [D loss: 0.207031, acc:  66%] [G loss: 14.691463] time: 0:04:20.652905\n",
            "[Epoch 0/5] [Batch 131/2400] [D loss: 0.197716, acc:  76%] [G loss: 20.951809] time: 0:04:21.942577\n",
            "[Epoch 0/5] [Batch 132/2400] [D loss: 0.214606, acc:  66%] [G loss: 14.688480] time: 0:04:23.235579\n",
            "[Epoch 0/5] [Batch 133/2400] [D loss: 0.194396, acc:  76%] [G loss: 14.708447] time: 0:04:24.507386\n",
            "[Epoch 0/5] [Batch 134/2400] [D loss: 0.186912, acc:  69%] [G loss: 14.692066] time: 0:04:25.783781\n",
            "[Epoch 0/5] [Batch 135/2400] [D loss: 0.178306, acc:  76%] [G loss: 14.695505] time: 0:04:27.068414\n",
            "[Epoch 0/5] [Batch 136/2400] [D loss: 0.175440, acc:  74%] [G loss: 14.694917] time: 0:04:28.350909\n",
            "[Epoch 0/5] [Batch 137/2400] [D loss: 0.173098, acc:  76%] [G loss: 14.698240] time: 0:04:29.635003\n",
            "[Epoch 0/5] [Batch 138/2400] [D loss: 0.170226, acc:  75%] [G loss: 14.695192] time: 0:04:30.903479\n",
            "[Epoch 0/5] [Batch 139/2400] [D loss: 0.163942, acc:  79%] [G loss: 20.063450] time: 0:04:32.173012\n",
            "[Epoch 0/5] [Batch 140/2400] [D loss: 0.181624, acc:  70%] [G loss: 14.692061] time: 0:04:33.455268\n",
            "[Epoch 0/5] [Batch 141/2400] [D loss: 0.177850, acc:  78%] [G loss: 20.387880] time: 0:04:34.719311\n",
            "[Epoch 0/5] [Batch 142/2400] [D loss: 0.206130, acc:  67%] [G loss: 14.687710] time: 0:04:35.988492\n",
            "[Epoch 0/5] [Batch 143/2400] [D loss: 0.182520, acc:  78%] [G loss: 14.714485] time: 0:04:37.243522\n",
            "[Epoch 0/5] [Batch 144/2400] [D loss: 0.178919, acc:  71%] [G loss: 14.691937] time: 0:04:38.507863\n",
            "[Epoch 0/5] [Batch 145/2400] [D loss: 0.165640, acc:  80%] [G loss: 14.710029] time: 0:04:39.780895\n",
            "[Epoch 0/5] [Batch 146/2400] [D loss: 0.153098, acc:  78%] [G loss: 21.266003] time: 0:04:41.048235\n",
            "[Epoch 0/5] [Batch 147/2400] [D loss: 0.172754, acc:  74%] [G loss: 14.698912] time: 0:04:42.315438\n",
            "[Epoch 0/5] [Batch 148/2400] [D loss: 0.159565, acc:  80%] [G loss: 14.709314] time: 0:04:43.696774\n",
            "[Epoch 0/5] [Batch 149/2400] [D loss: 0.156097, acc:  76%] [G loss: 14.699895] time: 0:04:44.962060\n",
            "[Epoch 0/5] [Batch 150/2400] [D loss: 0.150740, acc:  82%] [G loss: 14.711163] time: 0:04:46.247496\n",
            "[Epoch 0/5] [Batch 151/2400] [D loss: 0.140651, acc:  82%] [G loss: 20.274372] time: 0:04:47.507953\n",
            "[Epoch 0/5] [Batch 152/2400] [D loss: 0.151790, acc:  79%] [G loss: 14.704859] time: 0:04:48.774791\n",
            "[Epoch 0/5] [Batch 153/2400] [D loss: 0.144239, acc:  83%] [G loss: 14.712287] time: 0:04:50.055346\n",
            "[Epoch 0/5] [Batch 154/2400] [D loss: 0.142765, acc:  82%] [G loss: 14.705913] time: 0:04:51.319578\n",
            "[Epoch 0/5] [Batch 155/2400] [D loss: 0.145769, acc:  82%] [G loss: 21.125717] time: 0:04:52.583579\n",
            "[Epoch 0/5] [Batch 156/2400] [D loss: 0.201606, acc:  66%] [G loss: 14.698911] time: 0:04:53.863742\n",
            "[Epoch 0/5] [Batch 157/2400] [D loss: 0.178399, acc:  78%] [G loss: 14.743539] time: 0:04:55.127125\n",
            "[Epoch 0/5] [Batch 158/2400] [D loss: 0.170991, acc:  72%] [G loss: 14.702190] time: 0:04:56.396374\n",
            "[Epoch 0/5] [Batch 159/2400] [D loss: 0.201285, acc:  77%] [G loss: 20.834036] time: 0:04:57.661099\n",
            "[Epoch 0/5] [Batch 160/2400] [D loss: 0.324818, acc:  56%] [G loss: 14.704166] time: 0:04:58.941292\n",
            "[Epoch 0/5] [Batch 161/2400] [D loss: 0.287410, acc:  74%] [G loss: 14.787453] time: 0:05:00.223751\n",
            "[Epoch 0/5] [Batch 162/2400] [D loss: 0.273565, acc:  63%] [G loss: 14.715732] time: 0:05:01.500264\n",
            "[Epoch 0/5] [Batch 163/2400] [D loss: 0.234136, acc:  78%] [G loss: 14.773912] time: 0:05:02.768595\n",
            "[Epoch 0/5] [Batch 164/2400] [D loss: 0.189705, acc:  73%] [G loss: 21.096550] time: 0:05:04.047426\n",
            "[Epoch 0/5] [Batch 165/2400] [D loss: 0.157778, acc:  82%] [G loss: 14.744356] time: 0:05:05.328833\n",
            "[Epoch 0/5] [Batch 166/2400] [D loss: 0.141859, acc:  79%] [G loss: 14.725843] time: 0:05:06.692453\n",
            "[Epoch 0/5] [Batch 167/2400] [D loss: 0.143260, acc:  83%] [G loss: 20.587885] time: 0:05:07.952702\n",
            "[Epoch 0/5] [Batch 168/2400] [D loss: 0.189488, acc:  69%] [G loss: 14.723207] time: 0:05:09.220170\n",
            "[Epoch 0/5] [Batch 169/2400] [D loss: 0.201843, acc:  77%] [G loss: 21.135733] time: 0:05:10.497961\n",
            "[Epoch 0/5] [Batch 170/2400] [D loss: 0.277040, acc:  62%] [G loss: 14.715202] time: 0:05:11.762247\n",
            "[Epoch 0/5] [Batch 171/2400] [D loss: 0.248478, acc:  77%] [G loss: 14.793731] time: 0:05:13.041470\n",
            "[Epoch 0/5] [Batch 172/2400] [D loss: 0.241071, acc:  66%] [G loss: 14.721268] time: 0:05:14.308058\n",
            "[Epoch 0/5] [Batch 173/2400] [D loss: 0.206246, acc:  79%] [G loss: 14.778728] time: 0:05:15.584567\n",
            "[Epoch 0/5] [Batch 174/2400] [D loss: 0.184830, acc:  71%] [G loss: 14.728682] time: 0:05:16.850532\n",
            "[Epoch 0/5] [Batch 175/2400] [D loss: 0.156099, acc:  82%] [G loss: 14.767815] time: 0:05:18.110542\n",
            "[Epoch 0/5] [Batch 176/2400] [D loss: 0.153334, acc:  75%] [G loss: 14.732630] time: 0:05:19.389356\n",
            "[Epoch 0/5] [Batch 177/2400] [D loss: 0.164117, acc:  80%] [G loss: 20.634567] time: 0:05:20.669374\n",
            "[Epoch 0/5] [Batch 178/2400] [D loss: 0.215772, acc:  68%] [G loss: 14.726698] time: 0:05:21.951390\n",
            "[Epoch 0/5] [Batch 179/2400] [D loss: 0.181884, acc:  79%] [G loss: 14.782798] time: 0:05:23.280207\n",
            "[Epoch 0/5] [Batch 180/2400] [D loss: 0.162352, acc:  73%] [G loss: 14.732602] time: 0:05:24.620974\n",
            "[Epoch 0/5] [Batch 181/2400] [D loss: 0.141647, acc:  83%] [G loss: 14.766800] time: 0:05:25.962490\n",
            "[Epoch 0/5] [Batch 182/2400] [D loss: 0.136724, acc:  78%] [G loss: 14.742603] time: 0:05:27.318699\n",
            "[Epoch 0/5] [Batch 183/2400] [D loss: 0.135558, acc:  82%] [G loss: 21.233597] time: 0:05:28.662864\n",
            "[Epoch 0/5] [Batch 184/2400] [D loss: 0.159505, acc:  74%] [G loss: 14.740929] time: 0:05:30.024299\n",
            "[Epoch 0/5] [Batch 185/2400] [D loss: 0.139997, acc:  83%] [G loss: 14.775342] time: 0:05:31.422005\n",
            "[Epoch 0/5] [Batch 186/2400] [D loss: 0.137112, acc:  78%] [G loss: 14.743506] time: 0:05:32.704439\n",
            "[Epoch 0/5] [Batch 187/2400] [D loss: 0.123211, acc:  85%] [G loss: 14.773018] time: 0:05:33.963537\n",
            "[Epoch 0/5] [Batch 188/2400] [D loss: 0.115429, acc:  85%] [G loss: 14.750234] time: 0:05:35.225615\n",
            "[Epoch 0/5] [Batch 189/2400] [D loss: 0.107164, acc:  87%] [G loss: 14.773097] time: 0:05:36.485123\n",
            "[Epoch 0/5] [Batch 190/2400] [D loss: 0.109903, acc:  88%] [G loss: 14.751426] time: 0:05:37.746962\n",
            "[Epoch 0/5] [Batch 191/2400] [D loss: 0.106844, acc:  88%] [G loss: 14.769228] time: 0:05:39.008975\n",
            "[Epoch 0/5] [Batch 192/2400] [D loss: 0.112063, acc:  87%] [G loss: 14.753370] time: 0:05:40.289368\n",
            "[Epoch 0/5] [Batch 193/2400] [D loss: 0.107330, acc:  87%] [G loss: 14.775788] time: 0:05:41.534423\n",
            "[Epoch 0/5] [Batch 194/2400] [D loss: 0.113100, acc:  87%] [G loss: 14.752332] time: 0:05:42.776405\n",
            "[Epoch 0/5] [Batch 195/2400] [D loss: 0.114172, acc:  86%] [G loss: 14.787420] time: 0:05:44.040899\n",
            "[Epoch 0/5] [Batch 196/2400] [D loss: 0.124115, acc:  83%] [G loss: 14.754168] time: 0:05:45.299804\n",
            "[Epoch 0/5] [Batch 197/2400] [D loss: 0.123831, acc:  85%] [G loss: 14.788767] time: 0:05:46.545138\n",
            "[Epoch 0/5] [Batch 198/2400] [D loss: 0.115001, acc:  84%] [G loss: 21.135763] time: 0:05:47.812908\n",
            "[Epoch 0/5] [Batch 199/2400] [D loss: 0.106358, acc:  88%] [G loss: 14.784210] time: 0:05:49.089666\n",
            "[Epoch 0/5] [Batch 200/2400] [D loss: 0.091782, acc:  93%] [G loss: 14.710266] time: 0:05:50.355906\n",
            "(1, 512, 512, 1)\n",
            "[Epoch 0/5] [Batch 201/2400] [D loss: 0.087362, acc:  92%] [G loss: 14.717606] time: 0:05:55.822356\n",
            "[Epoch 0/5] [Batch 202/2400] [D loss: 0.090991, acc:  90%] [G loss: 21.041431] time: 0:05:57.084253\n",
            "[Epoch 0/5] [Batch 203/2400] [D loss: 0.125241, acc:  86%] [G loss: 14.697697] time: 0:05:58.309833\n",
            "[Epoch 0/5] [Batch 204/2400] [D loss: 0.120135, acc:  86%] [G loss: 14.733020] time: 0:05:59.530873\n",
            "[Epoch 0/5] [Batch 205/2400] [D loss: 0.113626, acc:  88%] [G loss: 14.699394] time: 0:06:00.759823\n",
            "[Epoch 0/5] [Batch 206/2400] [D loss: 0.113748, acc:  86%] [G loss: 14.738585] time: 0:06:02.002623\n",
            "[Epoch 0/5] [Batch 207/2400] [D loss: 0.117484, acc:  87%] [G loss: 14.704535] time: 0:06:03.243411\n",
            "[Epoch 0/5] [Batch 208/2400] [D loss: 0.108314, acc:  87%] [G loss: 14.740417] time: 0:06:04.486318\n",
            "[Epoch 0/5] [Batch 209/2400] [D loss: 0.111078, acc:  87%] [G loss: 14.706137] time: 0:06:05.715456\n",
            "[Epoch 0/5] [Batch 210/2400] [D loss: 0.110912, acc:  86%] [G loss: 14.749461] time: 0:06:06.992785\n",
            "[Epoch 0/5] [Batch 211/2400] [D loss: 0.121353, acc:  84%] [G loss: 14.705601] time: 0:06:08.257064\n",
            "[Epoch 0/5] [Batch 212/2400] [D loss: 0.121848, acc:  85%] [G loss: 14.751053] time: 0:06:09.524550\n",
            "[Epoch 0/5] [Batch 213/2400] [D loss: 0.135481, acc:  79%] [G loss: 14.706903] time: 0:06:10.804409\n",
            "[Epoch 0/5] [Batch 214/2400] [D loss: 0.137256, acc:  83%] [G loss: 14.758482] time: 0:06:12.065529\n",
            "[Epoch 0/5] [Batch 215/2400] [D loss: 0.152285, acc:  75%] [G loss: 14.708151] time: 0:06:13.327256\n",
            "[Epoch 0/5] [Batch 216/2400] [D loss: 0.151563, acc:  82%] [G loss: 14.761524] time: 0:06:14.589392\n",
            "[Epoch 0/5] [Batch 217/2400] [D loss: 0.156306, acc:  74%] [G loss: 14.712504] time: 0:06:15.848177\n",
            "[Epoch 0/5] [Batch 218/2400] [D loss: 0.156252, acc:  81%] [G loss: 14.758537] time: 0:06:17.111201\n",
            "[Epoch 0/5] [Batch 219/2400] [D loss: 0.161253, acc:  73%] [G loss: 14.713154] time: 0:06:18.375291\n",
            "[Epoch 0/5] [Batch 220/2400] [D loss: 0.203562, acc:  76%] [G loss: 20.242054] time: 0:06:19.652093\n",
            "[Epoch 0/5] [Batch 221/2400] [D loss: 0.254640, acc:  59%] [G loss: 20.622738] time: 0:06:20.896819\n",
            "[Epoch 0/5] [Batch 222/2400] [D loss: 0.254451, acc:  73%] [G loss: 14.707426] time: 0:06:22.156093\n",
            "[Epoch 0/5] [Batch 223/2400] [D loss: 0.154242, acc:  80%] [G loss: 14.647342] time: 0:06:23.421850\n",
            "[Epoch 0/5] [Batch 224/2400] [D loss: 0.262785, acc:  74%] [G loss: 20.900496] time: 0:06:24.665258\n",
            "[Epoch 0/5] [Batch 225/2400] [D loss: 0.235636, acc:  69%] [G loss: 14.528481] time: 0:06:25.943563\n",
            "[Epoch 0/5] [Batch 226/2400] [D loss: 0.209989, acc:  73%] [G loss: 14.038685] time: 0:06:27.247083\n",
            "[Epoch 0/5] [Batch 227/2400] [D loss: 0.315627, acc:  74%] [G loss: 13.539298] time: 0:06:28.549196\n",
            "[Epoch 0/5] [Batch 228/2400] [D loss: 0.359227, acc:  56%] [G loss: 13.255059] time: 0:06:29.855382\n",
            "[Epoch 0/5] [Batch 229/2400] [D loss: 0.167528, acc:  78%] [G loss: 12.385568] time: 0:06:31.159075\n",
            "[Epoch 0/5] [Batch 230/2400] [D loss: 0.124288, acc:  86%] [G loss: 11.392639] time: 0:06:32.438893\n",
            "[Epoch 0/5] [Batch 231/2400] [D loss: 0.306257, acc:  62%] [G loss: 10.749950] time: 0:06:33.709799\n",
            "[Epoch 0/5] [Batch 232/2400] [D loss: 0.178050, acc:  80%] [G loss: 10.195928] time: 0:06:34.966740\n",
            "[Epoch 0/5] [Batch 233/2400] [D loss: 0.168629, acc:  75%] [G loss: 9.012571] time: 0:06:36.246713\n",
            "[Epoch 0/5] [Batch 234/2400] [D loss: 0.184732, acc:  81%] [G loss: 9.245696] time: 0:06:37.508853\n",
            "[Epoch 0/5] [Batch 235/2400] [D loss: 0.389135, acc:  62%] [G loss: 9.235534] time: 0:06:38.780981\n",
            "[Epoch 0/5] [Batch 236/2400] [D loss: 0.489226, acc:  64%] [G loss: 9.451348] time: 0:06:40.054056\n",
            "[Epoch 0/5] [Batch 237/2400] [D loss: 0.548173, acc:  59%] [G loss: 8.468184] time: 0:06:41.335517\n",
            "[Epoch 0/5] [Batch 238/2400] [D loss: 0.621007, acc:  55%] [G loss: 8.287998] time: 0:06:42.577755\n",
            "[Epoch 0/5] [Batch 239/2400] [D loss: 0.186638, acc:  80%] [G loss: 8.047187] time: 0:06:43.919163\n",
            "[Epoch 0/5] [Batch 240/2400] [D loss: 0.324415, acc:  61%] [G loss: 7.181074] time: 0:06:45.181523\n",
            "[Epoch 0/5] [Batch 241/2400] [D loss: 0.223095, acc:  69%] [G loss: 7.887289] time: 0:06:46.443240\n",
            "[Epoch 0/5] [Batch 242/2400] [D loss: 0.316130, acc:  53%] [G loss: 11.849236] time: 0:06:47.706317\n",
            "[Epoch 0/5] [Batch 243/2400] [D loss: 0.320265, acc:  55%] [G loss: 11.575644] time: 0:06:48.989512\n",
            "[Epoch 0/5] [Batch 244/2400] [D loss: 0.261511, acc:  55%] [G loss: 9.458831] time: 0:06:50.268286\n",
            "[Epoch 0/5] [Batch 245/2400] [D loss: 0.159512, acc:  83%] [G loss: 7.581110] time: 0:06:51.547885\n",
            "[Epoch 0/5] [Batch 246/2400] [D loss: 0.235662, acc:  67%] [G loss: 8.240499] time: 0:06:52.818951\n",
            "[Epoch 0/5] [Batch 247/2400] [D loss: 0.280058, acc:  53%] [G loss: 7.522087] time: 0:06:54.097066\n",
            "[Epoch 0/5] [Batch 248/2400] [D loss: 0.298774, acc:  50%] [G loss: 6.780835] time: 0:06:55.362838\n",
            "[Epoch 0/5] [Batch 249/2400] [D loss: 0.308557, acc:  53%] [G loss: 7.168872] time: 0:06:56.642253\n",
            "[Epoch 0/5] [Batch 250/2400] [D loss: 0.303372, acc:  51%] [G loss: 6.983426] time: 0:06:57.924988\n",
            "[Epoch 0/5] [Batch 251/2400] [D loss: 0.290381, acc:  53%] [G loss: 10.229404] time: 0:06:59.193043\n",
            "[Epoch 0/5] [Batch 252/2400] [D loss: 0.293100, acc:  54%] [G loss: 8.946422] time: 0:07:00.455475\n",
            "[Epoch 0/5] [Batch 253/2400] [D loss: 0.288791, acc:  55%] [G loss: 9.145814] time: 0:07:01.716300\n",
            "[Epoch 0/5] [Batch 254/2400] [D loss: 0.289878, acc:  54%] [G loss: 9.294307] time: 0:07:03.000941\n",
            "[Epoch 0/5] [Batch 255/2400] [D loss: 0.289255, acc:  56%] [G loss: 8.721779] time: 0:07:04.269271\n",
            "[Epoch 0/5] [Batch 256/2400] [D loss: 0.292490, acc:  52%] [G loss: 7.962128] time: 0:07:05.533238\n",
            "[Epoch 0/5] [Batch 257/2400] [D loss: 0.291513, acc:  47%] [G loss: 10.822663] time: 0:07:06.888830\n",
            "[Epoch 0/5] [Batch 258/2400] [D loss: 0.289875, acc:  52%] [G loss: 7.051587] time: 0:07:08.171560\n",
            "[Epoch 0/5] [Batch 259/2400] [D loss: 0.286718, acc:  51%] [G loss: 6.980343] time: 0:07:09.436014\n",
            "[Epoch 0/5] [Batch 260/2400] [D loss: 0.289896, acc:  50%] [G loss: 6.539413] time: 0:07:10.717764\n",
            "[Epoch 0/5] [Batch 261/2400] [D loss: 0.289234, acc:  51%] [G loss: 6.481775] time: 0:07:12.002023\n",
            "[Epoch 0/5] [Batch 262/2400] [D loss: 0.287430, acc:  50%] [G loss: 6.198047] time: 0:07:13.269764\n",
            "[Epoch 0/5] [Batch 263/2400] [D loss: 0.284273, acc:  50%] [G loss: 5.983881] time: 0:07:14.516358\n",
            "[Epoch 0/5] [Batch 264/2400] [D loss: 0.283612, acc:  51%] [G loss: 6.496854] time: 0:07:15.793225\n",
            "[Epoch 0/5] [Batch 265/2400] [D loss: 0.283945, acc:  51%] [G loss: 5.961962] time: 0:07:17.077079\n",
            "[Epoch 0/5] [Batch 266/2400] [D loss: 0.276346, acc:  53%] [G loss: 8.968664] time: 0:07:18.370272\n",
            "[Epoch 0/5] [Batch 267/2400] [D loss: 0.282031, acc:  49%] [G loss: 7.378160] time: 0:07:19.623987\n",
            "[Epoch 0/5] [Batch 268/2400] [D loss: 0.279793, acc:  51%] [G loss: 6.304815] time: 0:07:20.884865\n",
            "[Epoch 0/5] [Batch 269/2400] [D loss: 0.279813, acc:  50%] [G loss: 6.575723] time: 0:07:22.159905\n",
            "[Epoch 0/5] [Batch 270/2400] [D loss: 0.278745, acc:  50%] [G loss: 6.207156] time: 0:07:23.448652\n",
            "[Epoch 0/5] [Batch 271/2400] [D loss: 0.272371, acc:  52%] [G loss: 9.715063] time: 0:07:24.711681\n",
            "[Epoch 0/5] [Batch 272/2400] [D loss: 0.276037, acc:  52%] [G loss: 6.933284] time: 0:07:25.993054\n",
            "[Epoch 0/5] [Batch 273/2400] [D loss: 0.277734, acc:  48%] [G loss: 6.815596] time: 0:07:27.255129\n",
            "[Epoch 0/5] [Batch 274/2400] [D loss: 0.276015, acc:  52%] [G loss: 5.934395] time: 0:07:28.519974\n",
            "[Epoch 0/5] [Batch 275/2400] [D loss: 0.277023, acc:  51%] [G loss: 6.338741] time: 0:07:29.784337\n",
            "[Epoch 0/5] [Batch 276/2400] [D loss: 0.269754, acc:  51%] [G loss: 9.268179] time: 0:07:31.099675\n",
            "[Epoch 0/5] [Batch 277/2400] [D loss: 0.273321, acc:  52%] [G loss: 6.179956] time: 0:07:32.382419\n",
            "[Epoch 0/5] [Batch 278/2400] [D loss: 0.271924, acc:  52%] [G loss: 6.256438] time: 0:07:33.647462\n",
            "[Epoch 0/5] [Batch 279/2400] [D loss: 0.271980, acc:  51%] [G loss: 5.996561] time: 0:07:34.945773\n",
            "[Epoch 0/5] [Batch 280/2400] [D loss: 0.264707, acc:  54%] [G loss: 10.779372] time: 0:07:36.224848\n",
            "[Epoch 0/5] [Batch 281/2400] [D loss: 0.272992, acc:  49%] [G loss: 6.360684] time: 0:07:37.506451\n",
            "[Epoch 0/5] [Batch 282/2400] [D loss: 0.273904, acc:  50%] [G loss: 6.345723] time: 0:07:38.768261\n",
            "[Epoch 0/5] [Batch 283/2400] [D loss: 0.271224, acc:  50%] [G loss: 6.500164] time: 0:07:40.049282\n",
            "[Epoch 0/5] [Batch 284/2400] [D loss: 0.266202, acc:  53%] [G loss: 9.295550] time: 0:07:41.311745\n",
            "[Epoch 0/5] [Batch 285/2400] [D loss: 0.269359, acc:  52%] [G loss: 6.147554] time: 0:07:42.595158\n",
            "[Epoch 0/5] [Batch 286/2400] [D loss: 0.265460, acc:  54%] [G loss: 8.285226] time: 0:07:43.862925\n",
            "[Epoch 0/5] [Batch 287/2400] [D loss: 0.265194, acc:  52%] [G loss: 8.279118] time: 0:07:45.156098\n",
            "[Epoch 0/5] [Batch 288/2400] [D loss: 0.267890, acc:  53%] [G loss: 6.779990] time: 0:07:46.421920\n",
            "[Epoch 0/5] [Batch 289/2400] [D loss: 0.269656, acc:  52%] [G loss: 6.267776] time: 0:07:47.701405\n",
            "[Epoch 0/5] [Batch 290/2400] [D loss: 0.271876, acc:  46%] [G loss: 6.654160] time: 0:07:48.983552\n",
            "[Epoch 0/5] [Batch 291/2400] [D loss: 0.271594, acc:  50%] [G loss: 6.328321] time: 0:07:50.263379\n",
            "[Epoch 0/5] [Batch 292/2400] [D loss: 0.270089, acc:  48%] [G loss: 5.857666] time: 0:07:51.527410\n",
            "[Epoch 0/5] [Batch 293/2400] [D loss: 0.270664, acc:  47%] [G loss: 5.950147] time: 0:07:52.790927\n",
            "[Epoch 0/5] [Batch 294/2400] [D loss: 0.270648, acc:  51%] [G loss: 5.983597] time: 0:07:54.090309\n",
            "[Epoch 0/5] [Batch 295/2400] [D loss: 0.268224, acc:  50%] [G loss: 5.912967] time: 0:07:55.354116\n",
            "[Epoch 0/5] [Batch 296/2400] [D loss: 0.266608, acc:  49%] [G loss: 8.617213] time: 0:07:56.617269\n",
            "[Epoch 0/5] [Batch 297/2400] [D loss: 0.267613, acc:  51%] [G loss: 6.054039] time: 0:07:57.915138\n",
            "[Epoch 0/5] [Batch 298/2400] [D loss: 0.266969, acc:  52%] [G loss: 5.884935] time: 0:07:59.202080\n",
            "[Epoch 0/5] [Batch 299/2400] [D loss: 0.268285, acc:  51%] [G loss: 6.359786] time: 0:08:00.505165\n",
            "[Epoch 0/5] [Batch 300/2400] [D loss: 0.262343, acc:  53%] [G loss: 7.715168] time: 0:08:01.771233\n",
            "[Epoch 0/5] [Batch 301/2400] [D loss: 0.265864, acc:  50%] [G loss: 7.730640] time: 0:08:03.051401\n",
            "[Epoch 0/5] [Batch 302/2400] [D loss: 0.256435, acc:  55%] [G loss: 10.504794] time: 0:08:04.331915\n",
            "[Epoch 0/5] [Batch 303/2400] [D loss: 0.262761, acc:  52%] [G loss: 7.930912] time: 0:08:05.597416\n",
            "[Epoch 0/5] [Batch 304/2400] [D loss: 0.260828, acc:  54%] [G loss: 7.722619] time: 0:08:06.861671\n",
            "[Epoch 0/5] [Batch 305/2400] [D loss: 0.262923, acc:  53%] [G loss: 7.395792] time: 0:08:08.126133\n",
            "[Epoch 0/5] [Batch 306/2400] [D loss: 0.260585, acc:  54%] [G loss: 9.762622] time: 0:08:09.405171\n",
            "[Epoch 0/5] [Batch 307/2400] [D loss: 0.263722, acc:  51%] [G loss: 7.765476] time: 0:08:10.672611\n",
            "[Epoch 0/5] [Batch 308/2400] [D loss: 0.262347, acc:  54%] [G loss: 7.191143] time: 0:08:11.947112\n",
            "[Epoch 0/5] [Batch 309/2400] [D loss: 0.259285, acc:  52%] [G loss: 7.161274] time: 0:08:13.223772\n",
            "[Epoch 0/5] [Batch 310/2400] [D loss: 0.262726, acc:  52%] [G loss: 6.921790] time: 0:08:14.504570\n",
            "[Epoch 0/5] [Batch 311/2400] [D loss: 0.259891, acc:  53%] [G loss: 6.930901] time: 0:08:15.780824\n",
            "[Epoch 0/5] [Batch 312/2400] [D loss: 0.258240, acc:  52%] [G loss: 6.773460] time: 0:08:17.072591\n",
            "[Epoch 0/5] [Batch 313/2400] [D loss: 0.262088, acc:  53%] [G loss: 7.476111] time: 0:08:18.474644\n",
            "[Epoch 0/5] [Batch 314/2400] [D loss: 0.260992, acc:  53%] [G loss: 6.797275] time: 0:08:19.739725\n",
            "[Epoch 0/5] [Batch 315/2400] [D loss: 0.258029, acc:  52%] [G loss: 6.729797] time: 0:08:21.021353\n",
            "[Epoch 0/5] [Batch 316/2400] [D loss: 0.257838, acc:  52%] [G loss: 6.628958] time: 0:08:22.304362\n",
            "[Epoch 0/5] [Batch 317/2400] [D loss: 0.259874, acc:  53%] [G loss: 7.212689] time: 0:08:23.585689\n",
            "[Epoch 0/5] [Batch 318/2400] [D loss: 0.259960, acc:  52%] [G loss: 6.782197] time: 0:08:24.853922\n",
            "[Epoch 0/5] [Batch 319/2400] [D loss: 0.259408, acc:  53%] [G loss: 6.686079] time: 0:08:26.141748\n",
            "[Epoch 0/5] [Batch 320/2400] [D loss: 0.257830, acc:  53%] [G loss: 9.711225] time: 0:08:27.409859\n",
            "[Epoch 0/5] [Batch 321/2400] [D loss: 0.258467, acc:  53%] [G loss: 7.055782] time: 0:08:28.680443\n",
            "[Epoch 0/5] [Batch 322/2400] [D loss: 0.257624, acc:  55%] [G loss: 6.770214] time: 0:08:29.953370\n",
            "[Epoch 0/5] [Batch 323/2400] [D loss: 0.257571, acc:  53%] [G loss: 6.514260] time: 0:08:31.238791\n",
            "[Epoch 0/5] [Batch 324/2400] [D loss: 0.256982, acc:  53%] [G loss: 9.995646] time: 0:08:32.516279\n",
            "[Epoch 0/5] [Batch 325/2400] [D loss: 0.258908, acc:  53%] [G loss: 6.682897] time: 0:08:33.799740\n",
            "[Epoch 0/5] [Batch 326/2400] [D loss: 0.258449, acc:  54%] [G loss: 7.310980] time: 0:08:35.067811\n",
            "[Epoch 0/5] [Batch 327/2400] [D loss: 0.258532, acc:  54%] [G loss: 7.217206] time: 0:08:36.342856\n",
            "[Epoch 0/5] [Batch 328/2400] [D loss: 0.258090, acc:  54%] [G loss: 10.004941] time: 0:08:37.624068\n",
            "[Epoch 0/5] [Batch 329/2400] [D loss: 0.255284, acc:  53%] [G loss: 7.039690] time: 0:08:38.888125\n",
            "[Epoch 0/5] [Batch 330/2400] [D loss: 0.254950, acc:  55%] [G loss: 7.051825] time: 0:08:40.168764\n",
            "[Epoch 0/5] [Batch 331/2400] [D loss: 0.252038, acc:  55%] [G loss: 6.952135] time: 0:08:41.490272\n",
            "[Epoch 0/5] [Batch 332/2400] [D loss: 0.254145, acc:  54%] [G loss: 6.561687] time: 0:08:42.756633\n",
            "[Epoch 0/5] [Batch 333/2400] [D loss: 0.255362, acc:  54%] [G loss: 6.525649] time: 0:08:44.034206\n",
            "[Epoch 0/5] [Batch 334/2400] [D loss: 0.258100, acc:  54%] [G loss: 7.189552] time: 0:08:45.298582\n",
            "[Epoch 0/5] [Batch 335/2400] [D loss: 0.258623, acc:  53%] [G loss: 7.192645] time: 0:08:46.559012\n",
            "[Epoch 0/5] [Batch 336/2400] [D loss: 0.255836, acc:  53%] [G loss: 6.616218] time: 0:08:47.821835\n",
            "[Epoch 0/5] [Batch 337/2400] [D loss: 0.255364, acc:  54%] [G loss: 6.855660] time: 0:08:49.087179\n",
            "[Epoch 0/5] [Batch 338/2400] [D loss: 0.252580, acc:  55%] [G loss: 6.585352] time: 0:08:50.368871\n",
            "[Epoch 0/5] [Batch 339/2400] [D loss: 0.253540, acc:  54%] [G loss: 6.414933] time: 0:08:51.650845\n",
            "[Epoch 0/5] [Batch 340/2400] [D loss: 0.252969, acc:  54%] [G loss: 6.300449] time: 0:08:52.922922\n",
            "[Epoch 0/5] [Batch 341/2400] [D loss: 0.252000, acc:  55%] [G loss: 6.202821] time: 0:08:54.200330\n",
            "[Epoch 0/5] [Batch 342/2400] [D loss: 0.254162, acc:  54%] [G loss: 6.615953] time: 0:08:55.479601\n",
            "[Epoch 0/5] [Batch 343/2400] [D loss: 0.250669, acc:  56%] [G loss: 6.139470] time: 0:08:56.759696\n",
            "[Epoch 0/5] [Batch 344/2400] [D loss: 0.255213, acc:  55%] [G loss: 6.596616] time: 0:08:58.024448\n",
            "[Epoch 0/5] [Batch 345/2400] [D loss: 0.254984, acc:  54%] [G loss: 6.358961] time: 0:08:59.308706\n",
            "[Epoch 0/5] [Batch 346/2400] [D loss: 0.256720, acc:  51%] [G loss: 10.623610] time: 0:09:00.579580\n",
            "[Epoch 0/5] [Batch 347/2400] [D loss: 0.253097, acc:  55%] [G loss: 6.544873] time: 0:09:01.871227\n",
            "[Epoch 0/5] [Batch 348/2400] [D loss: 0.253850, acc:  56%] [G loss: 8.281228] time: 0:09:03.132485\n",
            "[Epoch 0/5] [Batch 349/2400] [D loss: 0.250632, acc:  57%] [G loss: 7.843637] time: 0:09:04.414481\n",
            "[Epoch 0/5] [Batch 350/2400] [D loss: 0.252800, acc:  55%] [G loss: 6.393538] time: 0:09:05.834998\n",
            "[Epoch 0/5] [Batch 351/2400] [D loss: 0.256194, acc:  52%] [G loss: 6.971116] time: 0:09:07.117542\n",
            "[Epoch 0/5] [Batch 352/2400] [D loss: 0.258894, acc:  52%] [G loss: 7.295515] time: 0:09:08.376928\n",
            "[Epoch 0/5] [Batch 353/2400] [D loss: 0.254271, acc:  55%] [G loss: 6.259922] time: 0:09:09.655452\n",
            "[Epoch 0/5] [Batch 354/2400] [D loss: 0.250838, acc:  53%] [G loss: 6.474083] time: 0:09:10.940036\n",
            "[Epoch 0/5] [Batch 355/2400] [D loss: 0.250359, acc:  56%] [G loss: 11.185020] time: 0:09:12.202355\n",
            "[Epoch 0/5] [Batch 356/2400] [D loss: 0.250872, acc:  56%] [G loss: 6.881173] time: 0:09:13.482488\n",
            "[Epoch 0/5] [Batch 357/2400] [D loss: 0.250727, acc:  56%] [G loss: 7.136951] time: 0:09:14.747309\n",
            "[Epoch 0/5] [Batch 358/2400] [D loss: 0.250272, acc:  57%] [G loss: 9.230034] time: 0:09:16.039223\n",
            "[Epoch 0/5] [Batch 359/2400] [D loss: 0.250259, acc:  56%] [G loss: 7.084902] time: 0:09:17.310805\n",
            "[Epoch 0/5] [Batch 360/2400] [D loss: 0.250714, acc:  56%] [G loss: 6.430410] time: 0:09:18.594818\n",
            "[Epoch 0/5] [Batch 361/2400] [D loss: 0.254228, acc:  55%] [G loss: 6.619492] time: 0:09:19.881049\n",
            "[Epoch 0/5] [Batch 362/2400] [D loss: 0.254572, acc:  52%] [G loss: 6.832172] time: 0:09:21.164313\n",
            "[Epoch 0/5] [Batch 363/2400] [D loss: 0.253523, acc:  55%] [G loss: 6.274980] time: 0:09:22.428452\n",
            "[Epoch 0/5] [Batch 364/2400] [D loss: 0.252003, acc:  54%] [G loss: 6.146605] time: 0:09:23.702497\n",
            "[Epoch 0/5] [Batch 365/2400] [D loss: 0.251446, acc:  54%] [G loss: 5.978333] time: 0:09:24.976659\n",
            "[Epoch 0/5] [Batch 366/2400] [D loss: 0.251248, acc:  55%] [G loss: 5.915970] time: 0:09:26.273090\n",
            "[Epoch 0/5] [Batch 367/2400] [D loss: 0.249534, acc:  55%] [G loss: 6.346550] time: 0:09:27.539150\n",
            "[Epoch 0/5] [Batch 368/2400] [D loss: 0.247527, acc:  56%] [G loss: 10.916679] time: 0:09:28.908718\n",
            "[Epoch 0/5] [Batch 369/2400] [D loss: 0.248866, acc:  56%] [G loss: 7.131920] time: 0:09:30.171728\n",
            "[Epoch 0/5] [Batch 370/2400] [D loss: 0.249642, acc:  56%] [G loss: 6.516676] time: 0:09:31.452984\n",
            "[Epoch 0/5] [Batch 371/2400] [D loss: 0.247026, acc:  57%] [G loss: 6.487819] time: 0:09:32.717478\n",
            "[Epoch 0/5] [Batch 372/2400] [D loss: 0.257718, acc:  51%] [G loss: 11.070121] time: 0:09:33.992995\n",
            "[Epoch 0/5] [Batch 373/2400] [D loss: 0.256798, acc:  52%] [G loss: 10.110408] time: 0:09:35.258492\n",
            "[Epoch 0/5] [Batch 374/2400] [D loss: 0.256494, acc:  54%] [G loss: 7.862873] time: 0:09:36.559967\n",
            "[Epoch 0/5] [Batch 375/2400] [D loss: 0.245914, acc:  58%] [G loss: 7.640925] time: 0:09:37.812712\n",
            "[Epoch 0/5] [Batch 376/2400] [D loss: 0.241285, acc:  61%] [G loss: 7.778962] time: 0:09:39.092742\n",
            "[Epoch 0/5] [Batch 377/2400] [D loss: 0.245034, acc:  57%] [G loss: 6.669685] time: 0:09:40.354439\n",
            "[Epoch 0/5] [Batch 378/2400] [D loss: 0.253865, acc:  53%] [G loss: 7.096573] time: 0:09:41.649007\n",
            "[Epoch 0/5] [Batch 379/2400] [D loss: 0.256328, acc:  52%] [G loss: 6.694184] time: 0:09:42.914666\n",
            "[Epoch 0/5] [Batch 380/2400] [D loss: 0.254688, acc:  52%] [G loss: 6.905997] time: 0:09:44.193960\n",
            "[Epoch 0/5] [Batch 381/2400] [D loss: 0.253167, acc:  54%] [G loss: 6.059752] time: 0:09:45.460425\n",
            "[Epoch 0/5] [Batch 382/2400] [D loss: 0.252748, acc:  54%] [G loss: 6.539881] time: 0:09:46.741328\n",
            "[Epoch 0/5] [Batch 383/2400] [D loss: 0.249998, acc:  55%] [G loss: 6.632442] time: 0:09:48.024180\n",
            "[Epoch 0/5] [Batch 384/2400] [D loss: 0.251838, acc:  56%] [G loss: 6.419360] time: 0:09:49.307058\n",
            "[Epoch 0/5] [Batch 385/2400] [D loss: 0.247913, acc:  57%] [G loss: 6.137499] time: 0:09:50.586854\n",
            "[Epoch 0/5] [Batch 386/2400] [D loss: 0.253126, acc:  52%] [G loss: 9.559441] time: 0:09:51.869864\n",
            "[Epoch 0/5] [Batch 387/2400] [D loss: 0.246094, acc:  55%] [G loss: 6.555714] time: 0:09:53.172565\n",
            "[Epoch 0/5] [Batch 388/2400] [D loss: 0.245932, acc:  58%] [G loss: 6.868394] time: 0:09:54.453534\n",
            "[Epoch 0/5] [Batch 389/2400] [D loss: 0.246110, acc:  59%] [G loss: 9.438952] time: 0:09:55.715114\n",
            "[Epoch 0/5] [Batch 390/2400] [D loss: 0.245897, acc:  57%] [G loss: 6.558839] time: 0:09:56.984016\n",
            "[Epoch 0/5] [Batch 391/2400] [D loss: 0.247510, acc:  57%] [G loss: 9.998770] time: 0:09:58.242523\n",
            "[Epoch 0/5] [Batch 392/2400] [D loss: 0.246842, acc:  57%] [G loss: 7.377658] time: 0:09:59.521762\n",
            "[Epoch 0/5] [Batch 393/2400] [D loss: 0.249639, acc:  57%] [G loss: 6.728655] time: 0:10:00.790047\n",
            "[Epoch 0/5] [Batch 394/2400] [D loss: 0.255786, acc:  55%] [G loss: 6.482255] time: 0:10:02.067596\n",
            "[Epoch 0/5] [Batch 395/2400] [D loss: 0.257515, acc:  49%] [G loss: 6.420074] time: 0:10:03.347861\n",
            "[Epoch 0/5] [Batch 396/2400] [D loss: 0.253238, acc:  51%] [G loss: 6.385897] time: 0:10:04.627967\n",
            "[Epoch 0/5] [Batch 397/2400] [D loss: 0.251407, acc:  56%] [G loss: 6.201515] time: 0:10:05.893580\n",
            "[Epoch 0/5] [Batch 398/2400] [D loss: 0.250172, acc:  56%] [G loss: 5.949572] time: 0:10:07.193647\n",
            "[Epoch 0/5] [Batch 399/2400] [D loss: 0.248268, acc:  53%] [G loss: 6.145989] time: 0:10:08.457749\n",
            "[Epoch 0/5] [Batch 400/2400] [D loss: 0.256673, acc:  47%] [G loss: 9.063922] time: 0:10:09.756684\n",
            "(1, 512, 512, 1)\n",
            "[Epoch 0/5] [Batch 401/2400] [D loss: 0.258271, acc:  50%] [G loss: 7.491269] time: 0:10:15.036638\n",
            "[Epoch 0/5] [Batch 402/2400] [D loss: 0.260042, acc:  49%] [G loss: 6.943533] time: 0:10:16.261679\n",
            "[Epoch 0/5] [Batch 403/2400] [D loss: 0.270480, acc:  43%] [G loss: 7.016308] time: 0:10:17.482048\n",
            "[Epoch 0/5] [Batch 404/2400] [D loss: 0.273932, acc:  40%] [G loss: 6.597593] time: 0:10:18.724534\n",
            "[Epoch 0/5] [Batch 405/2400] [D loss: 0.266468, acc:  45%] [G loss: 6.772633] time: 0:10:19.946912\n",
            "[Epoch 0/5] [Batch 406/2400] [D loss: 0.267987, acc:  45%] [G loss: 6.675494] time: 0:10:21.189697\n",
            "[Epoch 0/5] [Batch 407/2400] [D loss: 0.270575, acc:  41%] [G loss: 6.219178] time: 0:10:22.414704\n",
            "[Epoch 0/5] [Batch 408/2400] [D loss: 0.267570, acc:  43%] [G loss: 6.204578] time: 0:10:23.656740\n",
            "[Epoch 0/5] [Batch 409/2400] [D loss: 0.260271, acc:  46%] [G loss: 8.691686] time: 0:10:24.899082\n",
            "[Epoch 0/5] [Batch 410/2400] [D loss: 0.261910, acc:  47%] [G loss: 6.742127] time: 0:10:26.143992\n",
            "[Epoch 0/5] [Batch 411/2400] [D loss: 0.265026, acc:  43%] [G loss: 6.161691] time: 0:10:27.405045\n",
            "[Epoch 0/5] [Batch 412/2400] [D loss: 0.263989, acc:  43%] [G loss: 6.266763] time: 0:10:28.670254\n",
            "[Epoch 0/5] [Batch 413/2400] [D loss: 0.268484, acc:  41%] [G loss: 6.845502] time: 0:10:29.936680\n",
            "[Epoch 0/5] [Batch 414/2400] [D loss: 0.264726, acc:  42%] [G loss: 5.868634] time: 0:10:31.219700\n",
            "[Epoch 0/5] [Batch 415/2400] [D loss: 0.266561, acc:  45%] [G loss: 6.378202] time: 0:10:32.484686\n",
            "[Epoch 0/5] [Batch 416/2400] [D loss: 0.264582, acc:  46%] [G loss: 5.801691] time: 0:10:33.841086\n",
            "[Epoch 0/5] [Batch 417/2400] [D loss: 0.257066, acc:  48%] [G loss: 8.548309] time: 0:10:35.170134\n",
            "[Epoch 0/5] [Batch 418/2400] [D loss: 0.264165, acc:  43%] [G loss: 6.524352] time: 0:10:36.513119\n",
            "[Epoch 0/5] [Batch 419/2400] [D loss: 0.262503, acc:  43%] [G loss: 5.991776] time: 0:10:37.855199\n",
            "[Epoch 0/5] [Batch 420/2400] [D loss: 0.267692, acc:  43%] [G loss: 6.214577] time: 0:10:39.199369\n",
            "[Epoch 0/5] [Batch 421/2400] [D loss: 0.269403, acc:  40%] [G loss: 6.572803] time: 0:10:40.539696\n",
            "[Epoch 0/5] [Batch 422/2400] [D loss: 0.268199, acc:  40%] [G loss: 5.986050] time: 0:10:41.885653\n",
            "[Epoch 0/5] [Batch 423/2400] [D loss: 0.265447, acc:  43%] [G loss: 6.088515] time: 0:10:43.186855\n",
            "[Epoch 0/5] [Batch 424/2400] [D loss: 0.267013, acc:  42%] [G loss: 5.833015] time: 0:10:44.449910\n",
            "[Epoch 0/5] [Batch 425/2400] [D loss: 0.267172, acc:  41%] [G loss: 6.021075] time: 0:10:45.730121\n",
            "[Epoch 0/5] [Batch 426/2400] [D loss: 0.266909, acc:  42%] [G loss: 5.849609] time: 0:10:47.017082\n",
            "[Epoch 0/5] [Batch 427/2400] [D loss: 0.266127, acc:  42%] [G loss: 5.827707] time: 0:10:48.277339\n",
            "[Epoch 0/5] [Batch 428/2400] [D loss: 0.266213, acc:  43%] [G loss: 5.732406] time: 0:10:49.577926\n",
            "[Epoch 0/5] [Batch 429/2400] [D loss: 0.257722, acc:  49%] [G loss: 8.804393] time: 0:10:50.838705\n",
            "[Epoch 0/5] [Batch 430/2400] [D loss: 0.259989, acc:  53%] [G loss: 6.220458] time: 0:10:52.111388\n",
            "[Epoch 0/5] [Batch 431/2400] [D loss: 0.261114, acc:  46%] [G loss: 6.031249] time: 0:10:53.381674\n",
            "[Epoch 0/5] [Batch 432/2400] [D loss: 0.261624, acc:  46%] [G loss: 5.821760] time: 0:10:54.682894\n",
            "[Epoch 0/5] [Batch 433/2400] [D loss: 0.262334, acc:  46%] [G loss: 5.746010] time: 0:10:55.965393\n",
            "[Epoch 0/5] [Batch 434/2400] [D loss: 0.262004, acc:  47%] [G loss: 5.604873] time: 0:10:57.231965\n",
            "[Epoch 0/5] [Batch 435/2400] [D loss: 0.262717, acc:  48%] [G loss: 5.436514] time: 0:10:58.492224\n",
            "[Epoch 0/5] [Batch 436/2400] [D loss: 0.265767, acc:  44%] [G loss: 6.377662] time: 0:10:59.774696\n",
            "[Epoch 0/5] [Batch 437/2400] [D loss: 0.266168, acc:  41%] [G loss: 6.372065] time: 0:11:01.044835\n",
            "[Epoch 0/5] [Batch 438/2400] [D loss: 0.262855, acc:  44%] [G loss: 5.881069] time: 0:11:02.338496\n",
            "[Epoch 0/5] [Batch 439/2400] [D loss: 0.260875, acc:  50%] [G loss: 5.774147] time: 0:11:03.611853\n",
            "[Epoch 0/5] [Batch 440/2400] [D loss: 0.264114, acc:  49%] [G loss: 5.762091] time: 0:11:04.884687\n",
            "[Epoch 0/5] [Batch 441/2400] [D loss: 0.261526, acc:  47%] [G loss: 5.451636] time: 0:11:06.196248\n",
            "[Epoch 0/5] [Batch 442/2400] [D loss: 0.260535, acc:  49%] [G loss: 5.298740] time: 0:11:07.494224\n",
            "[Epoch 0/5] [Batch 443/2400] [D loss: 0.253494, acc:  50%] [G loss: 8.737275] time: 0:11:08.774118\n",
            "[Epoch 0/5] [Batch 444/2400] [D loss: 0.258422, acc:  48%] [G loss: 5.411690] time: 0:11:10.073150\n",
            "[Epoch 0/5] [Batch 445/2400] [D loss: 0.264569, acc:  44%] [G loss: 7.559985] time: 0:11:11.338416\n",
            "[Epoch 0/5] [Batch 446/2400] [D loss: 0.261068, acc:  48%] [G loss: 6.318852] time: 0:11:12.620736\n",
            "[Epoch 0/5] [Batch 447/2400] [D loss: 0.256337, acc:  48%] [G loss: 6.919491] time: 0:11:13.942379\n",
            "[Epoch 0/5] [Batch 448/2400] [D loss: 0.262042, acc:  44%] [G loss: 6.531863] time: 0:11:15.223078\n",
            "[Epoch 0/5] [Batch 449/2400] [D loss: 0.265295, acc:  42%] [G loss: 5.893651] time: 0:11:16.526680\n",
            "[Epoch 0/5] [Batch 450/2400] [D loss: 0.262913, acc:  48%] [G loss: 5.779576] time: 0:11:17.808995\n",
            "[Epoch 0/5] [Batch 451/2400] [D loss: 0.264794, acc:  47%] [G loss: 5.659551] time: 0:11:19.115369\n",
            "[Epoch 0/5] [Batch 452/2400] [D loss: 0.255026, acc:  49%] [G loss: 7.958971] time: 0:11:20.391726\n",
            "[Epoch 0/5] [Batch 453/2400] [D loss: 0.258583, acc:  48%] [G loss: 5.829330] time: 0:11:21.675878\n",
            "[Epoch 0/5] [Batch 454/2400] [D loss: 0.258457, acc:  49%] [G loss: 6.203948] time: 0:11:22.948387\n",
            "[Epoch 0/5] [Batch 455/2400] [D loss: 0.260050, acc:  45%] [G loss: 5.672460] time: 0:11:24.260435\n",
            "[Epoch 0/5] [Batch 456/2400] [D loss: 0.258773, acc:  47%] [G loss: 5.315357] time: 0:11:25.540625\n",
            "[Epoch 0/5] [Batch 457/2400] [D loss: 0.264122, acc:  44%] [G loss: 6.235624] time: 0:11:26.824752\n",
            "[Epoch 0/5] [Batch 458/2400] [D loss: 0.259640, acc:  45%] [G loss: 5.133024] time: 0:11:28.105189\n",
            "[Epoch 0/5] [Batch 459/2400] [D loss: 0.260567, acc:  47%] [G loss: 5.410337] time: 0:11:29.372653\n",
            "[Epoch 0/5] [Batch 460/2400] [D loss: 0.257416, acc:  47%] [G loss: 6.106162] time: 0:11:30.685640\n",
            "[Epoch 0/5] [Batch 461/2400] [D loss: 0.255378, acc:  48%] [G loss: 8.502883] time: 0:11:31.957886\n",
            "[Epoch 0/5] [Batch 462/2400] [D loss: 0.248994, acc:  54%] [G loss: 8.161023] time: 0:11:33.279076\n",
            "[Epoch 0/5] [Batch 463/2400] [D loss: 0.257617, acc:  54%] [G loss: 6.253148] time: 0:11:34.584074\n",
            "[Epoch 0/5] [Batch 464/2400] [D loss: 0.256734, acc:  53%] [G loss: 6.004764] time: 0:11:35.868775\n",
            "[Epoch 0/5] [Batch 465/2400] [D loss: 0.259743, acc:  47%] [G loss: 6.457652] time: 0:11:37.151320\n",
            "[Epoch 0/5] [Batch 466/2400] [D loss: 0.253681, acc:  48%] [G loss: 7.990288] time: 0:11:38.434580\n",
            "[Epoch 0/5] [Batch 467/2400] [D loss: 0.250261, acc:  50%] [G loss: 7.186201] time: 0:11:39.707293\n",
            "[Epoch 0/5] [Batch 468/2400] [D loss: 0.252285, acc:  51%] [G loss: 6.529195] time: 0:11:40.986539\n",
            "[Epoch 0/5] [Batch 469/2400] [D loss: 0.246329, acc:  53%] [G loss: 7.729413] time: 0:11:42.250780\n",
            "[Epoch 0/5] [Batch 470/2400] [D loss: 0.259257, acc:  54%] [G loss: 6.011528] time: 0:11:43.530899\n",
            "[Epoch 0/5] [Batch 471/2400] [D loss: 0.255512, acc:  55%] [G loss: 5.976293] time: 0:11:44.798635\n",
            "[Epoch 0/5] [Batch 472/2400] [D loss: 0.249985, acc:  53%] [G loss: 8.296968] time: 0:11:46.101621\n",
            "[Epoch 0/5] [Batch 473/2400] [D loss: 0.248591, acc:  57%] [G loss: 7.492964] time: 0:11:47.362563\n",
            "[Epoch 0/5] [Batch 474/2400] [D loss: 0.252904, acc:  54%] [G loss: 5.984324] time: 0:11:48.664526\n",
            "[Epoch 0/5] [Batch 475/2400] [D loss: 0.261517, acc:  47%] [G loss: 6.610057] time: 0:11:49.938740\n",
            "[Epoch 0/5] [Batch 476/2400] [D loss: 0.261385, acc:  43%] [G loss: 5.529475] time: 0:11:51.231560\n",
            "[Epoch 0/5] [Batch 477/2400] [D loss: 0.261279, acc:  45%] [G loss: 5.266545] time: 0:11:52.498923\n",
            "[Epoch 0/5] [Batch 478/2400] [D loss: 0.259498, acc:  51%] [G loss: 5.009553] time: 0:11:53.937476\n",
            "[Epoch 0/5] [Batch 479/2400] [D loss: 0.260448, acc:  49%] [G loss: 5.243236] time: 0:11:55.217578\n",
            "[Epoch 0/5] [Batch 480/2400] [D loss: 0.256693, acc:  48%] [G loss: 4.929821] time: 0:11:56.484599\n",
            "[Epoch 0/5] [Batch 481/2400] [D loss: 0.249619, acc:  52%] [G loss: 9.048295] time: 0:11:57.752412\n",
            "[Epoch 0/5] [Batch 482/2400] [D loss: 0.254905, acc:  48%] [G loss: 5.693365] time: 0:11:59.026978\n",
            "[Epoch 0/5] [Batch 483/2400] [D loss: 0.249695, acc:  49%] [G loss: 7.706604] time: 0:12:00.308434\n",
            "[Epoch 0/5] [Batch 484/2400] [D loss: 0.251901, acc:  47%] [G loss: 5.469120] time: 0:12:01.575126\n",
            "[Epoch 0/5] [Batch 485/2400] [D loss: 0.254976, acc:  56%] [G loss: 5.285841] time: 0:12:02.837145\n",
            "[Epoch 0/5] [Batch 486/2400] [D loss: 0.260143, acc:  57%] [G loss: 6.411023] time: 0:12:04.136577\n",
            "[Epoch 0/5] [Batch 487/2400] [D loss: 0.257562, acc:  57%] [G loss: 5.710303] time: 0:12:05.399034\n",
            "[Epoch 0/5] [Batch 488/2400] [D loss: 0.257700, acc:  55%] [G loss: 4.926260] time: 0:12:06.681085\n",
            "[Epoch 0/5] [Batch 489/2400] [D loss: 0.250567, acc:  52%] [G loss: 8.515080] time: 0:12:07.962256\n",
            "[Epoch 0/5] [Batch 490/2400] [D loss: 0.248407, acc:  50%] [G loss: 7.071639] time: 0:12:09.252808\n",
            "[Epoch 0/5] [Batch 491/2400] [D loss: 0.252131, acc:  51%] [G loss: 6.503933] time: 0:12:10.527914\n",
            "[Epoch 0/5] [Batch 492/2400] [D loss: 0.254296, acc:  50%] [G loss: 5.724508] time: 0:12:11.811594\n",
            "[Epoch 0/5] [Batch 493/2400] [D loss: 0.252410, acc:  53%] [G loss: 5.182995] time: 0:12:13.090633\n",
            "[Epoch 0/5] [Batch 494/2400] [D loss: 0.256066, acc:  54%] [G loss: 4.959662] time: 0:12:14.386531\n",
            "[Epoch 0/5] [Batch 495/2400] [D loss: 0.259315, acc:  52%] [G loss: 6.044531] time: 0:12:15.672319\n",
            "[Epoch 0/5] [Batch 496/2400] [D loss: 0.255941, acc:  53%] [G loss: 4.785671] time: 0:12:16.951957\n",
            "[Epoch 0/5] [Batch 497/2400] [D loss: 0.255661, acc:  52%] [G loss: 4.793052] time: 0:12:18.255951\n",
            "[Epoch 0/5] [Batch 498/2400] [D loss: 0.255069, acc:  49%] [G loss: 5.854908] time: 0:12:19.526352\n",
            "[Epoch 0/5] [Batch 499/2400] [D loss: 0.256144, acc:  45%] [G loss: 5.885719] time: 0:12:20.799816\n",
            "[Epoch 0/5] [Batch 500/2400] [D loss: 0.237759, acc:  56%] [G loss: 6.876464] time: 0:12:22.077610\n",
            "[Epoch 0/5] [Batch 501/2400] [D loss: 0.238585, acc:  61%] [G loss: 6.905678] time: 0:12:23.351571\n",
            "[Epoch 0/5] [Batch 502/2400] [D loss: 0.250836, acc:  55%] [G loss: 6.399732] time: 0:12:24.643280\n",
            "[Epoch 0/5] [Batch 503/2400] [D loss: 0.251750, acc:  58%] [G loss: 6.751275] time: 0:12:25.926226\n",
            "[Epoch 0/5] [Batch 504/2400] [D loss: 0.267890, acc:  54%] [G loss: 6.615817] time: 0:12:27.197675\n",
            "[Epoch 0/5] [Batch 505/2400] [D loss: 0.442532, acc:  50%] [G loss: 6.334685] time: 0:12:28.487818\n",
            "[Epoch 0/5] [Batch 506/2400] [D loss: 0.316148, acc:  46%] [G loss: 6.438478] time: 0:12:29.750756\n",
            "[Epoch 0/5] [Batch 507/2400] [D loss: 0.814991, acc:  40%] [G loss: 7.224893] time: 0:12:31.034108\n",
            "[Epoch 0/5] [Batch 508/2400] [D loss: 0.980107, acc:  35%] [G loss: 6.010353] time: 0:12:32.317686\n",
            "[Epoch 0/5] [Batch 509/2400] [D loss: 0.663448, acc:  49%] [G loss: 10.781660] time: 0:12:33.583723\n",
            "[Epoch 0/5] [Batch 510/2400] [D loss: 0.855243, acc:  44%] [G loss: 7.003982] time: 0:12:34.851491\n",
            "[Epoch 0/5] [Batch 511/2400] [D loss: 0.475157, acc:  45%] [G loss: 7.088333] time: 0:12:36.129029\n",
            "[Epoch 0/5] [Batch 512/2400] [D loss: 0.317923, acc:  62%] [G loss: 6.669259] time: 0:12:37.399586\n",
            "[Epoch 0/5] [Batch 513/2400] [D loss: 0.273840, acc:  52%] [G loss: 6.639567] time: 0:12:38.675583\n",
            "[Epoch 0/5] [Batch 514/2400] [D loss: 0.253486, acc:  48%] [G loss: 8.693186] time: 0:12:39.938150\n",
            "[Epoch 0/5] [Batch 515/2400] [D loss: 0.243426, acc:  61%] [G loss: 6.092528] time: 0:12:41.243079\n",
            "[Epoch 0/5] [Batch 516/2400] [D loss: 0.244157, acc:  53%] [G loss: 6.149545] time: 0:12:42.512174\n",
            "[Epoch 0/5] [Batch 517/2400] [D loss: 0.238935, acc:  55%] [G loss: 6.001447] time: 0:12:43.792782\n",
            "[Epoch 0/5] [Batch 518/2400] [D loss: 0.246341, acc:  54%] [G loss: 9.635084] time: 0:12:45.076807\n",
            "[Epoch 0/5] [Batch 519/2400] [D loss: 0.237831, acc:  63%] [G loss: 6.119976] time: 0:12:46.361277\n",
            "[Epoch 0/5] [Batch 520/2400] [D loss: 0.240490, acc:  56%] [G loss: 8.043958] time: 0:12:47.624677\n",
            "[Epoch 0/5] [Batch 521/2400] [D loss: 0.234000, acc:  63%] [G loss: 6.094047] time: 0:12:48.909075\n",
            "[Epoch 0/5] [Batch 522/2400] [D loss: 0.236436, acc:  60%] [G loss: 6.083410] time: 0:12:50.190908\n",
            "[Epoch 0/5] [Batch 523/2400] [D loss: 0.237306, acc:  55%] [G loss: 6.287593] time: 0:12:51.465390\n",
            "[Epoch 0/5] [Batch 524/2400] [D loss: 0.235461, acc:  61%] [G loss: 5.615045] time: 0:12:52.739119\n",
            "[Epoch 0/5] [Batch 525/2400] [D loss: 0.235065, acc:  63%] [G loss: 5.926464] time: 0:12:54.022107\n",
            "[Epoch 0/5] [Batch 526/2400] [D loss: 0.235102, acc:  62%] [G loss: 6.086916] time: 0:12:55.303357\n",
            "[Epoch 0/5] [Batch 527/2400] [D loss: 0.235567, acc:  62%] [G loss: 5.674705] time: 0:12:56.586760\n",
            "[Epoch 0/5] [Batch 528/2400] [D loss: 0.234776, acc:  63%] [G loss: 6.576105] time: 0:12:57.863837\n",
            "[Epoch 0/5] [Batch 529/2400] [D loss: 0.244524, acc:  55%] [G loss: 9.338964] time: 0:12:59.152669\n",
            "[Epoch 0/5] [Batch 530/2400] [D loss: 0.234847, acc:  62%] [G loss: 6.442248] time: 0:13:00.435189\n",
            "[Epoch 0/5] [Batch 531/2400] [D loss: 0.231984, acc:  61%] [G loss: 6.342383] time: 0:13:01.704409\n",
            "[Epoch 0/5] [Batch 532/2400] [D loss: 0.238415, acc:  54%] [G loss: 8.947342] time: 0:13:02.984447\n",
            "[Epoch 0/5] [Batch 533/2400] [D loss: 0.233017, acc:  62%] [G loss: 6.335538] time: 0:13:04.280796\n",
            "[Epoch 0/5] [Batch 534/2400] [D loss: 0.232398, acc:  59%] [G loss: 6.048751] time: 0:13:05.600673\n",
            "[Epoch 0/5] [Batch 535/2400] [D loss: 0.229786, acc:  63%] [G loss: 5.710707] time: 0:13:06.880467\n",
            "[Epoch 0/5] [Batch 536/2400] [D loss: 0.231027, acc:  64%] [G loss: 5.606428] time: 0:13:08.145367\n",
            "[Epoch 0/5] [Batch 537/2400] [D loss: 0.231349, acc:  65%] [G loss: 5.537290] time: 0:13:09.427774\n",
            "[Epoch 0/5] [Batch 538/2400] [D loss: 0.231139, acc:  65%] [G loss: 5.942084] time: 0:13:10.690104\n",
            "[Epoch 0/5] [Batch 539/2400] [D loss: 0.231898, acc:  64%] [G loss: 6.005074] time: 0:13:11.953178\n",
            "[Epoch 0/5] [Batch 540/2400] [D loss: 0.230571, acc:  65%] [G loss: 5.549176] time: 0:13:13.234688\n",
            "[Epoch 0/5] [Batch 541/2400] [D loss: 0.230038, acc:  65%] [G loss: 5.535243] time: 0:13:14.503663\n",
            "[Epoch 0/5] [Batch 542/2400] [D loss: 0.231860, acc:  64%] [G loss: 6.499695] time: 0:13:15.781860\n",
            "[Epoch 0/5] [Batch 543/2400] [D loss: 0.230974, acc:  65%] [G loss: 5.873901] time: 0:13:17.061520\n",
            "[Epoch 0/5] [Batch 544/2400] [D loss: 0.229870, acc:  65%] [G loss: 5.380858] time: 0:13:18.332814\n",
            "[Epoch 0/5] [Batch 545/2400] [D loss: 0.228578, acc:  66%] [G loss: 5.787681] time: 0:13:19.608712\n",
            "[Epoch 0/5] [Batch 546/2400] [D loss: 0.228782, acc:  66%] [G loss: 5.337595] time: 0:13:20.888169\n",
            "[Epoch 0/5] [Batch 547/2400] [D loss: 0.235518, acc:  63%] [G loss: 8.373847] time: 0:13:22.171063\n",
            "[Epoch 0/5] [Batch 548/2400] [D loss: 0.226525, acc:  65%] [G loss: 5.370163] time: 0:13:23.452390\n",
            "[Epoch 0/5] [Batch 549/2400] [D loss: 0.226696, acc:  66%] [G loss: 6.333779] time: 0:13:24.717609\n",
            "[Epoch 0/5] [Batch 550/2400] [D loss: 0.226266, acc:  66%] [G loss: 5.610994] time: 0:13:25.998644\n",
            "[Epoch 0/5] [Batch 551/2400] [D loss: 0.226607, acc:  64%] [G loss: 5.185868] time: 0:13:27.281404\n",
            "[Epoch 0/5] [Batch 552/2400] [D loss: 0.226865, acc:  66%] [G loss: 5.924100] time: 0:13:28.704095\n",
            "[Epoch 0/5] [Batch 553/2400] [D loss: 0.229817, acc:  60%] [G loss: 8.509798] time: 0:13:29.983986\n",
            "[Epoch 0/5] [Batch 554/2400] [D loss: 0.224722, acc:  63%] [G loss: 6.325911] time: 0:13:31.280551\n",
            "[Epoch 0/5] [Batch 555/2400] [D loss: 0.232971, acc:  63%] [G loss: 8.076022] time: 0:13:32.569238\n",
            "[Epoch 0/5] [Batch 556/2400] [D loss: 0.221070, acc:  68%] [G loss: 5.884851] time: 0:13:33.854308\n",
            "[Epoch 0/5] [Batch 557/2400] [D loss: 0.223126, acc:  67%] [G loss: 5.354273] time: 0:13:35.123559\n",
            "[Epoch 0/5] [Batch 558/2400] [D loss: 0.236447, acc:  57%] [G loss: 7.167004] time: 0:13:36.397283\n",
            "[Epoch 0/5] [Batch 559/2400] [D loss: 0.221101, acc:  65%] [G loss: 5.940268] time: 0:13:37.659476\n",
            "[Epoch 0/5] [Batch 560/2400] [D loss: 0.223280, acc:  66%] [G loss: 6.325930] time: 0:13:38.949999\n",
            "[Epoch 0/5] [Batch 561/2400] [D loss: 0.222387, acc:  68%] [G loss: 5.178390] time: 0:13:40.224177\n",
            "[Epoch 0/5] [Batch 562/2400] [D loss: 0.223482, acc:  67%] [G loss: 5.841695] time: 0:13:41.486935\n",
            "[Epoch 0/5] [Batch 563/2400] [D loss: 0.224844, acc:  67%] [G loss: 8.243832] time: 0:13:42.756078\n",
            "[Epoch 0/5] [Batch 564/2400] [D loss: 0.220757, acc:  65%] [G loss: 5.858108] time: 0:13:44.045409\n",
            "[Epoch 0/5] [Batch 565/2400] [D loss: 0.222158, acc:  68%] [G loss: 5.800963] time: 0:13:45.314684\n",
            "[Epoch 0/5] [Batch 566/2400] [D loss: 0.226322, acc:  62%] [G loss: 7.822032] time: 0:13:46.595398\n",
            "[Epoch 0/5] [Batch 567/2400] [D loss: 0.224478, acc:  62%] [G loss: 5.837060] time: 0:13:47.859843\n",
            "[Epoch 0/5] [Batch 568/2400] [D loss: 0.223843, acc:  66%] [G loss: 5.489235] time: 0:13:49.127593\n",
            "[Epoch 0/5] [Batch 569/2400] [D loss: 0.236129, acc:  59%] [G loss: 6.898819] time: 0:13:50.403640\n",
            "[Epoch 0/5] [Batch 570/2400] [D loss: 0.225798, acc:  61%] [G loss: 7.482206] time: 0:13:51.670458\n",
            "[Epoch 0/5] [Batch 571/2400] [D loss: 0.223134, acc:  61%] [G loss: 5.658216] time: 0:13:53.002299\n",
            "[Epoch 0/5] [Batch 572/2400] [D loss: 0.230333, acc:  66%] [G loss: 7.998652] time: 0:13:54.295955\n",
            "[Epoch 0/5] [Batch 573/2400] [D loss: 0.221885, acc:  68%] [G loss: 6.239420] time: 0:13:55.564069\n",
            "[Epoch 0/5] [Batch 574/2400] [D loss: 0.226559, acc:  60%] [G loss: 6.061151] time: 0:13:56.848723\n",
            "[Epoch 0/5] [Batch 575/2400] [D loss: 0.223955, acc:  65%] [G loss: 5.541591] time: 0:13:58.124813\n",
            "[Epoch 0/5] [Batch 576/2400] [D loss: 0.233395, acc:  60%] [G loss: 5.678235] time: 0:13:59.434206\n",
            "[Epoch 0/5] [Batch 577/2400] [D loss: 0.228319, acc:  65%] [G loss: 8.270329] time: 0:14:00.719189\n",
            "[Epoch 0/5] [Batch 578/2400] [D loss: 0.219700, acc:  67%] [G loss: 5.733364] time: 0:14:02.003773\n",
            "[Epoch 0/5] [Batch 579/2400] [D loss: 0.220084, acc:  65%] [G loss: 5.550035] time: 0:14:03.287845\n",
            "[Epoch 0/5] [Batch 580/2400] [D loss: 0.219893, acc:  67%] [G loss: 5.275320] time: 0:14:04.567832\n",
            "[Epoch 0/5] [Batch 581/2400] [D loss: 0.219912, acc:  68%] [G loss: 5.355844] time: 0:14:05.848819\n",
            "[Epoch 0/5] [Batch 582/2400] [D loss: 0.221168, acc:  67%] [G loss: 5.198198] time: 0:14:07.132418\n",
            "[Epoch 0/5] [Batch 583/2400] [D loss: 0.224688, acc:  65%] [G loss: 6.008349] time: 0:14:08.416399\n",
            "[Epoch 0/5] [Batch 584/2400] [D loss: 0.220030, acc:  69%] [G loss: 5.108557] time: 0:14:09.682137\n",
            "[Epoch 0/5] [Batch 585/2400] [D loss: 0.221500, acc:  66%] [G loss: 5.727232] time: 0:14:10.960672\n",
            "[Epoch 0/5] [Batch 586/2400] [D loss: 0.240785, acc:  57%] [G loss: 7.409934] time: 0:14:12.229094\n",
            "[Epoch 0/5] [Batch 587/2400] [D loss: 0.218380, acc:  68%] [G loss: 5.651346] time: 0:14:13.506808\n",
            "[Epoch 0/5] [Batch 588/2400] [D loss: 0.218806, acc:  64%] [G loss: 5.413589] time: 0:14:14.788759\n",
            "[Epoch 0/5] [Batch 589/2400] [D loss: 0.224717, acc:  63%] [G loss: 5.484321] time: 0:14:16.071623\n",
            "[Epoch 0/5] [Batch 590/2400] [D loss: 0.222042, acc:  67%] [G loss: 5.407677] time: 0:14:17.371816\n",
            "[Epoch 0/5] [Batch 591/2400] [D loss: 0.226618, acc:  59%] [G loss: 5.595852] time: 0:14:18.657741\n",
            "[Epoch 0/5] [Batch 592/2400] [D loss: 0.223738, acc:  66%] [G loss: 5.456223] time: 0:14:19.938526\n",
            "[Epoch 0/5] [Batch 593/2400] [D loss: 0.226775, acc:  62%] [G loss: 5.105196] time: 0:14:21.221113\n",
            "[Epoch 0/5] [Batch 594/2400] [D loss: 0.226047, acc:  63%] [G loss: 5.304719] time: 0:14:22.501528\n",
            "[Epoch 0/5] [Batch 595/2400] [D loss: 0.224991, acc:  63%] [G loss: 8.180090] time: 0:14:23.785036\n",
            "[Epoch 0/5] [Batch 596/2400] [D loss: 0.232316, acc:  61%] [G loss: 6.774240] time: 0:14:25.066338\n",
            "[Epoch 0/5] [Batch 597/2400] [D loss: 0.212422, acc:  69%] [G loss: 5.853476] time: 0:14:26.348908\n",
            "[Epoch 0/5] [Batch 598/2400] [D loss: 0.217652, acc:  71%] [G loss: 6.935555] time: 0:14:27.628557\n",
            "[Epoch 0/5] [Batch 599/2400] [D loss: 0.218957, acc:  63%] [G loss: 5.915908] time: 0:14:28.893641\n",
            "[Epoch 0/5] [Batch 600/2400] [D loss: 0.195724, acc:  76%] [G loss: 7.765019] time: 0:14:30.175821\n",
            "(1, 512, 512, 1)\n",
            "[Epoch 0/5] [Batch 601/2400] [D loss: 0.229210, acc:  68%] [G loss: 8.310423] time: 0:14:35.993682\n",
            "[Epoch 0/5] [Batch 602/2400] [D loss: 0.209044, acc:  68%] [G loss: 7.246256] time: 0:14:37.235752\n",
            "[Epoch 0/5] [Batch 603/2400] [D loss: 0.235608, acc:  63%] [G loss: 7.751570] time: 0:14:38.457319\n",
            "[Epoch 0/5] [Batch 604/2400] [D loss: 0.247988, acc:  60%] [G loss: 6.917610] time: 0:14:39.678764\n",
            "[Epoch 0/5] [Batch 605/2400] [D loss: 0.225035, acc:  60%] [G loss: 6.783499] time: 0:14:40.920411\n",
            "[Epoch 0/5] [Batch 606/2400] [D loss: 0.444880, acc:  43%] [G loss: 7.030762] time: 0:14:42.160901\n",
            "[Epoch 0/5] [Batch 607/2400] [D loss: 0.296297, acc:  66%] [G loss: 9.423805] time: 0:14:43.467902\n",
            "[Epoch 0/5] [Batch 608/2400] [D loss: 0.502342, acc:  60%] [G loss: 6.920366] time: 0:14:44.727487\n",
            "[Epoch 0/5] [Batch 609/2400] [D loss: 0.567282, acc:  53%] [G loss: 6.943753] time: 0:14:45.969678\n",
            "[Epoch 0/5] [Batch 610/2400] [D loss: 0.451996, acc:  42%] [G loss: 6.631089] time: 0:14:47.249822\n",
            "[Epoch 0/5] [Batch 611/2400] [D loss: 0.507742, acc:  52%] [G loss: 5.989720] time: 0:14:48.533979\n",
            "[Epoch 0/5] [Batch 612/2400] [D loss: 0.384406, acc:  58%] [G loss: 6.799014] time: 0:14:49.815974\n",
            "[Epoch 0/5] [Batch 613/2400] [D loss: 0.311343, acc:  56%] [G loss: 5.670320] time: 0:14:51.080388\n",
            "[Epoch 0/5] [Batch 614/2400] [D loss: 0.294069, acc:  46%] [G loss: 8.557968] time: 0:14:52.342324\n",
            "[Epoch 0/5] [Batch 615/2400] [D loss: 0.307952, acc:  63%] [G loss: 6.762241] time: 0:14:53.604742\n",
            "[Epoch 0/5] [Batch 616/2400] [D loss: 0.298562, acc:  59%] [G loss: 6.117133] time: 0:14:54.863360\n",
            "[Epoch 0/5] [Batch 617/2400] [D loss: 0.236927, acc:  54%] [G loss: 8.945313] time: 0:14:56.120757\n",
            "[Epoch 0/5] [Batch 618/2400] [D loss: 0.248613, acc:  62%] [G loss: 6.715456] time: 0:14:57.396325\n",
            "[Epoch 0/5] [Batch 619/2400] [D loss: 0.224380, acc:  62%] [G loss: 6.362028] time: 0:14:58.680487\n",
            "[Epoch 0/5] [Batch 620/2400] [D loss: 0.212627, acc:  60%] [G loss: 6.906649] time: 0:14:59.943866\n",
            "[Epoch 0/5] [Batch 621/2400] [D loss: 0.222130, acc:  58%] [G loss: 6.458140] time: 0:15:01.226048\n",
            "[Epoch 0/5] [Batch 622/2400] [D loss: 0.207054, acc:  71%] [G loss: 6.192881] time: 0:15:02.501120\n",
            "[Epoch 0/5] [Batch 623/2400] [D loss: 0.204121, acc:  73%] [G loss: 5.928068] time: 0:15:03.774867\n",
            "[Epoch 0/5] [Batch 624/2400] [D loss: 0.198641, acc:  77%] [G loss: 6.363526] time: 0:15:05.057461\n",
            "[Epoch 0/5] [Batch 625/2400] [D loss: 0.202709, acc:  76%] [G loss: 7.875854] time: 0:15:06.325851\n",
            "[Epoch 0/5] [Batch 626/2400] [D loss: 0.198295, acc:  74%] [G loss: 5.687109] time: 0:15:07.607891\n",
            "[Epoch 0/5] [Batch 627/2400] [D loss: 0.205608, acc:  75%] [G loss: 9.266273] time: 0:15:08.890277\n",
            "[Epoch 0/5] [Batch 628/2400] [D loss: 0.214031, acc:  75%] [G loss: 7.307574] time: 0:15:10.159433\n",
            "[Epoch 0/5] [Batch 629/2400] [D loss: 0.192742, acc:  74%] [G loss: 6.893884] time: 0:15:11.456982\n",
            "[Epoch 0/5] [Batch 630/2400] [D loss: 0.197319, acc:  69%] [G loss: 6.830177] time: 0:15:12.738475\n",
            "[Epoch 0/5] [Batch 631/2400] [D loss: 0.200614, acc:  74%] [G loss: 5.999504] time: 0:15:14.019633\n",
            "[Epoch 0/5] [Batch 632/2400] [D loss: 0.201575, acc:  76%] [G loss: 5.557093] time: 0:15:15.282704\n",
            "[Epoch 0/5] [Batch 633/2400] [D loss: 0.196761, acc:  78%] [G loss: 6.140883] time: 0:15:16.545471\n",
            "[Epoch 0/5] [Batch 634/2400] [D loss: 0.194351, acc:  70%] [G loss: 5.325899] time: 0:15:17.824683\n",
            "[Epoch 0/5] [Batch 635/2400] [D loss: 0.193335, acc:  76%] [G loss: 6.347126] time: 0:15:19.106117\n",
            "[Epoch 0/5] [Batch 636/2400] [D loss: 0.195822, acc:  78%] [G loss: 5.406913] time: 0:15:20.385938\n",
            "[Epoch 0/5] [Batch 637/2400] [D loss: 0.195911, acc:  71%] [G loss: 5.239307] time: 0:15:21.649064\n",
            "[Epoch 0/5] [Batch 638/2400] [D loss: 0.205288, acc:  63%] [G loss: 5.586733] time: 0:15:22.929270\n",
            "[Epoch 0/5] [Batch 639/2400] [D loss: 0.199787, acc:  73%] [G loss: 5.050823] time: 0:15:24.193400\n",
            "[Epoch 0/5] [Batch 640/2400] [D loss: 0.206798, acc:  68%] [G loss: 4.925407] time: 0:15:25.474579\n",
            "[Epoch 0/5] [Batch 641/2400] [D loss: 0.198388, acc:  71%] [G loss: 4.824434] time: 0:15:26.754383\n",
            "[Epoch 0/5] [Batch 642/2400] [D loss: 0.207101, acc:  63%] [G loss: 4.730948] time: 0:15:28.037117\n",
            "[Epoch 0/5] [Batch 643/2400] [D loss: 0.201711, acc:  71%] [G loss: 6.169881] time: 0:15:29.317720\n",
            "[Epoch 0/5] [Batch 644/2400] [D loss: 0.203390, acc:  65%] [G loss: 4.697599] time: 0:15:30.719922\n",
            "[Epoch 0/5] [Batch 645/2400] [D loss: 0.192910, acc:  70%] [G loss: 5.820153] time: 0:15:31.981593\n",
            "[Epoch 0/5] [Batch 646/2400] [D loss: 0.212413, acc:  61%] [G loss: 5.063675] time: 0:15:33.269551\n",
            "[Epoch 0/5] [Batch 647/2400] [D loss: 0.197792, acc:  73%] [G loss: 4.828897] time: 0:15:34.545409\n",
            "[Epoch 0/5] [Batch 648/2400] [D loss: 0.219924, acc:  60%] [G loss: 6.359210] time: 0:15:35.811482\n",
            "[Epoch 0/5] [Batch 649/2400] [D loss: 0.196139, acc:  70%] [G loss: 6.064753] time: 0:15:37.093119\n",
            "[Epoch 0/5] [Batch 650/2400] [D loss: 0.245748, acc:  55%] [G loss: 6.136540] time: 0:15:38.372430\n",
            "[Epoch 0/5] [Batch 651/2400] [D loss: 0.209486, acc:  68%] [G loss: 5.293122] time: 0:15:39.642491\n",
            "[Epoch 0/5] [Batch 652/2400] [D loss: 0.242786, acc:  67%] [G loss: 8.206231] time: 0:15:40.921410\n",
            "[Epoch 0/5] [Batch 653/2400] [D loss: 0.212094, acc:  61%] [G loss: 5.629653] time: 0:15:42.201251\n",
            "[Epoch 0/5] [Batch 654/2400] [D loss: 0.221662, acc:  63%] [G loss: 6.091137] time: 0:15:43.503183\n",
            "[Epoch 0/5] [Batch 655/2400] [D loss: 0.213030, acc:  69%] [G loss: 8.462921] time: 0:15:44.806729\n",
            "[Epoch 0/5] [Batch 656/2400] [D loss: 0.180666, acc:  81%] [G loss: 5.781729] time: 0:15:46.170611\n",
            "[Epoch 0/5] [Batch 657/2400] [D loss: 0.194317, acc:  72%] [G loss: 5.500485] time: 0:15:47.513168\n",
            "[Epoch 0/5] [Batch 658/2400] [D loss: 0.191610, acc:  71%] [G loss: 4.811720] time: 0:15:48.857847\n",
            "[Epoch 0/5] [Batch 659/2400] [D loss: 0.228190, acc:  60%] [G loss: 10.002270] time: 0:15:50.223367\n",
            "[Epoch 0/5] [Batch 660/2400] [D loss: 0.232350, acc:  70%] [G loss: 6.876347] time: 0:15:51.582186\n",
            "[Epoch 0/5] [Batch 661/2400] [D loss: 0.229601, acc:  67%] [G loss: 6.317186] time: 0:15:52.944265\n",
            "[Epoch 0/5] [Batch 662/2400] [D loss: 0.281556, acc:  58%] [G loss: 6.248997] time: 0:15:54.209299\n",
            "[Epoch 0/5] [Batch 663/2400] [D loss: 0.291552, acc:  61%] [G loss: 6.079237] time: 0:15:55.509015\n",
            "[Epoch 0/5] [Batch 664/2400] [D loss: 0.347259, acc:  56%] [G loss: 6.106475] time: 0:15:56.791348\n",
            "[Epoch 0/5] [Batch 665/2400] [D loss: 0.293487, acc:  55%] [G loss: 5.328706] time: 0:15:58.056545\n",
            "[Epoch 0/5] [Batch 666/2400] [D loss: 0.300077, acc:  65%] [G loss: 8.991920] time: 0:15:59.320426\n",
            "[Epoch 0/5] [Batch 667/2400] [D loss: 0.191575, acc:  72%] [G loss: 6.007804] time: 0:16:00.601964\n",
            "[Epoch 0/5] [Batch 668/2400] [D loss: 0.204005, acc:  69%] [G loss: 6.778214] time: 0:16:01.884822\n",
            "[Epoch 0/5] [Batch 669/2400] [D loss: 0.226437, acc:  64%] [G loss: 6.115237] time: 0:16:03.170556\n",
            "[Epoch 0/5] [Batch 670/2400] [D loss: 0.275872, acc:  50%] [G loss: 6.048206] time: 0:16:04.448411\n",
            "[Epoch 0/5] [Batch 671/2400] [D loss: 0.268228, acc:  45%] [G loss: 5.647986] time: 0:16:05.718811\n",
            "[Epoch 0/5] [Batch 672/2400] [D loss: 0.222160, acc:  66%] [G loss: 4.878944] time: 0:16:07.003181\n",
            "[Epoch 0/5] [Batch 673/2400] [D loss: 0.215952, acc:  68%] [G loss: 5.077028] time: 0:16:08.275290\n",
            "[Epoch 0/5] [Batch 674/2400] [D loss: 0.212757, acc:  71%] [G loss: 4.611956] time: 0:16:09.552056\n",
            "[Epoch 0/5] [Batch 675/2400] [D loss: 0.205617, acc:  74%] [G loss: 6.025003] time: 0:16:10.816954\n",
            "[Epoch 0/5] [Batch 676/2400] [D loss: 0.206111, acc:  73%] [G loss: 4.541784] time: 0:16:12.087746\n",
            "[Epoch 0/5] [Batch 677/2400] [D loss: 0.213804, acc:  71%] [G loss: 6.038869] time: 0:16:13.366001\n",
            "[Epoch 0/5] [Batch 678/2400] [D loss: 0.213663, acc:  70%] [G loss: 4.652253] time: 0:16:14.636916\n",
            "[Epoch 0/5] [Batch 679/2400] [D loss: 0.197056, acc:  75%] [G loss: 7.427070] time: 0:16:15.913565\n",
            "[Epoch 0/5] [Batch 680/2400] [D loss: 0.232881, acc:  61%] [G loss: 4.795018] time: 0:16:17.179370\n",
            "[Epoch 0/5] [Batch 681/2400] [D loss: 0.252557, acc:  54%] [G loss: 5.524896] time: 0:16:18.498900\n",
            "[Epoch 0/5] [Batch 682/2400] [D loss: 0.232414, acc:  52%] [G loss: 4.831249] time: 0:16:19.784034\n",
            "[Epoch 0/5] [Batch 683/2400] [D loss: 0.215485, acc:  71%] [G loss: 4.574207] time: 0:16:21.068907\n",
            "[Epoch 0/5] [Batch 684/2400] [D loss: 0.222812, acc:  65%] [G loss: 4.716722] time: 0:16:22.350810\n",
            "[Epoch 0/5] [Batch 685/2400] [D loss: 0.217493, acc:  66%] [G loss: 6.121616] time: 0:16:23.631960\n",
            "[Epoch 0/5] [Batch 686/2400] [D loss: 0.215595, acc:  64%] [G loss: 5.298684] time: 0:16:24.915939\n",
            "[Epoch 0/5] [Batch 687/2400] [D loss: 0.216062, acc:  71%] [G loss: 7.988657] time: 0:16:26.198189\n",
            "[Epoch 0/5] [Batch 688/2400] [D loss: 0.211404, acc:  70%] [G loss: 6.092688] time: 0:16:27.481162\n",
            "[Epoch 0/5] [Batch 689/2400] [D loss: 0.229984, acc:  63%] [G loss: 5.222924] time: 0:16:28.743230\n",
            "[Epoch 0/5] [Batch 690/2400] [D loss: 0.205699, acc:  77%] [G loss: 8.362371] time: 0:16:30.011916\n",
            "[Epoch 0/5] [Batch 691/2400] [D loss: 0.208888, acc:  69%] [G loss: 4.933649] time: 0:16:31.287045\n",
            "[Epoch 0/5] [Batch 692/2400] [D loss: 0.215069, acc:  71%] [G loss: 6.599248] time: 0:16:32.568631\n",
            "[Epoch 0/5] [Batch 693/2400] [D loss: 0.211654, acc:  67%] [G loss: 5.946607] time: 0:16:33.850190\n",
            "[Epoch 0/5] [Batch 694/2400] [D loss: 0.211362, acc:  69%] [G loss: 5.371830] time: 0:16:35.132203\n",
            "[Epoch 0/5] [Batch 695/2400] [D loss: 0.221091, acc:  63%] [G loss: 5.067986] time: 0:16:36.412374\n",
            "[Epoch 0/5] [Batch 696/2400] [D loss: 0.210385, acc:  71%] [G loss: 4.626875] time: 0:16:37.719013\n",
            "[Epoch 0/5] [Batch 697/2400] [D loss: 0.221886, acc:  62%] [G loss: 5.074563] time: 0:16:39.061614\n",
            "[Epoch 0/5] [Batch 698/2400] [D loss: 0.225167, acc:  62%] [G loss: 5.463487] time: 0:16:40.414908\n",
            "[Epoch 0/5] [Batch 699/2400] [D loss: 0.227669, acc:  62%] [G loss: 4.770225] time: 0:16:41.742051\n",
            "[Epoch 0/5] [Batch 700/2400] [D loss: 0.286004, acc:  45%] [G loss: 5.912884] time: 0:16:43.133895\n",
            "[Epoch 0/5] [Batch 701/2400] [D loss: 0.297725, acc:  48%] [G loss: 5.351142] time: 0:16:44.406520\n",
            "[Epoch 0/5] [Batch 702/2400] [D loss: 0.307599, acc:  40%] [G loss: 6.439029] time: 0:16:45.687881\n",
            "[Epoch 0/5] [Batch 703/2400] [D loss: 0.329719, acc:  35%] [G loss: 8.110171] time: 0:16:46.950300\n",
            "[Epoch 0/5] [Batch 704/2400] [D loss: 0.287642, acc:  37%] [G loss: 5.956312] time: 0:16:48.229961\n",
            "[Epoch 0/5] [Batch 705/2400] [D loss: 0.272112, acc:  42%] [G loss: 5.821520] time: 0:16:49.512523\n",
            "[Epoch 0/5] [Batch 706/2400] [D loss: 0.285194, acc:  34%] [G loss: 7.946912] time: 0:16:50.796937\n",
            "[Epoch 0/5] [Batch 707/2400] [D loss: 0.265547, acc:  41%] [G loss: 6.068324] time: 0:16:52.079109\n",
            "[Epoch 0/5] [Batch 708/2400] [D loss: 0.266537, acc:  44%] [G loss: 5.496243] time: 0:16:53.360734\n",
            "[Epoch 0/5] [Batch 709/2400] [D loss: 0.276617, acc:  39%] [G loss: 5.263168] time: 0:16:54.624212\n",
            "[Epoch 0/5] [Batch 710/2400] [D loss: 0.279858, acc:  36%] [G loss: 5.744182] time: 0:16:55.903456\n",
            "[Epoch 0/5] [Batch 711/2400] [D loss: 0.274227, acc:  38%] [G loss: 5.180105] time: 0:16:57.164582\n",
            "[Epoch 0/5] [Batch 712/2400] [D loss: 0.269417, acc:  42%] [G loss: 5.124272] time: 0:16:58.427585\n",
            "[Epoch 0/5] [Batch 713/2400] [D loss: 0.267076, acc:  43%] [G loss: 4.994608] time: 0:16:59.696842\n",
            "[Epoch 0/5] [Batch 714/2400] [D loss: 0.266841, acc:  42%] [G loss: 4.879974] time: 0:17:00.976586\n",
            "[Epoch 0/5] [Batch 715/2400] [D loss: 0.268940, acc:  39%] [G loss: 5.528966] time: 0:17:02.241339\n",
            "[Epoch 0/5] [Batch 716/2400] [D loss: 0.266126, acc:  43%] [G loss: 4.888867] time: 0:17:03.521639\n",
            "[Epoch 0/5] [Batch 717/2400] [D loss: 0.282129, acc:  38%] [G loss: 7.623486] time: 0:17:04.784648\n",
            "[Epoch 0/5] [Batch 718/2400] [D loss: 0.265550, acc:  39%] [G loss: 5.522933] time: 0:17:06.088379\n",
            "[Epoch 0/5] [Batch 719/2400] [D loss: 0.266584, acc:  46%] [G loss: 7.262004] time: 0:17:07.332336\n",
            "[Epoch 0/5] [Batch 720/2400] [D loss: 0.273546, acc:  42%] [G loss: 7.164298] time: 0:17:08.588538\n",
            "[Epoch 0/5] [Batch 721/2400] [D loss: 0.264768, acc:  40%] [G loss: 6.330865] time: 0:17:09.877961\n",
            "[Epoch 0/5] [Batch 722/2400] [D loss: 0.263965, acc:  41%] [G loss: 5.610957] time: 0:17:11.161004\n",
            "[Epoch 0/5] [Batch 723/2400] [D loss: 0.290474, acc:  31%] [G loss: 7.165731] time: 0:17:12.433744\n",
            "[Epoch 0/5] [Batch 724/2400] [D loss: 0.268561, acc:  49%] [G loss: 5.540855] time: 0:17:13.710308\n",
            "[Epoch 0/5] [Batch 725/2400] [D loss: 0.287212, acc:  47%] [G loss: 7.985476] time: 0:17:14.993007\n",
            "[Epoch 0/5] [Batch 726/2400] [D loss: 0.269335, acc:  42%] [G loss: 5.552538] time: 0:17:16.276306\n",
            "[Epoch 0/5] [Batch 727/2400] [D loss: 0.279798, acc:  43%] [G loss: 7.307046] time: 0:17:17.542018\n",
            "[Epoch 0/5] [Batch 728/2400] [D loss: 0.264038, acc:  42%] [G loss: 5.698919] time: 0:17:18.804156\n",
            "[Epoch 0/5] [Batch 729/2400] [D loss: 0.271431, acc:  37%] [G loss: 5.617054] time: 0:17:20.076119\n",
            "[Epoch 0/5] [Batch 730/2400] [D loss: 0.272697, acc:  39%] [G loss: 5.241993] time: 0:17:21.354897\n",
            "[Epoch 0/5] [Batch 731/2400] [D loss: 0.280024, acc:  37%] [G loss: 5.435591] time: 0:17:22.635406\n",
            "[Epoch 0/5] [Batch 732/2400] [D loss: 0.279357, acc:  37%] [G loss: 5.125885] time: 0:17:23.923894\n",
            "[Epoch 0/5] [Batch 733/2400] [D loss: 0.287085, acc:  47%] [G loss: 8.176603] time: 0:17:25.207219\n",
            "[Epoch 0/5] [Batch 734/2400] [D loss: 0.269823, acc:  44%] [G loss: 5.602145] time: 0:17:26.490460\n",
            "[Epoch 0/5] [Batch 735/2400] [D loss: 0.273391, acc:  36%] [G loss: 5.649779] time: 0:17:27.754681\n",
            "[Epoch 0/5] [Batch 736/2400] [D loss: 0.278704, acc:  34%] [G loss: 5.273705] time: 0:17:29.035492\n",
            "[Epoch 0/5] [Batch 737/2400] [D loss: 0.279007, acc:  37%] [G loss: 5.116552] time: 0:17:30.321668\n",
            "[Epoch 0/5] [Batch 738/2400] [D loss: 0.274032, acc:  35%] [G loss: 5.210236] time: 0:17:31.578385\n",
            "[Epoch 0/5] [Batch 739/2400] [D loss: 0.271808, acc:  38%] [G loss: 5.215665] time: 0:17:32.840263\n",
            "[Epoch 0/5] [Batch 740/2400] [D loss: 0.273539, acc:  37%] [G loss: 5.277351] time: 0:17:34.127403\n",
            "[Epoch 0/5] [Batch 741/2400] [D loss: 0.271210, acc:  41%] [G loss: 5.049967] time: 0:17:35.403617\n",
            "[Epoch 0/5] [Batch 742/2400] [D loss: 0.272206, acc:  39%] [G loss: 5.542443] time: 0:17:36.682630\n",
            "[Epoch 0/5] [Batch 743/2400] [D loss: 0.272270, acc:  39%] [G loss: 5.317003] time: 0:17:37.964861\n",
            "[Epoch 0/5] [Batch 744/2400] [D loss: 0.288002, acc:  43%] [G loss: 7.451407] time: 0:17:39.244443\n",
            "[Epoch 0/5] [Batch 745/2400] [D loss: 0.287456, acc:  53%] [G loss: 7.715559] time: 0:17:40.506783\n",
            "[Epoch 0/5] [Batch 746/2400] [D loss: 0.276304, acc:  50%] [G loss: 6.857189] time: 0:17:41.790847\n",
            "[Epoch 0/5] [Batch 747/2400] [D loss: 0.268255, acc:  37%] [G loss: 6.170065] time: 0:17:43.059021\n",
            "[Epoch 0/5] [Batch 748/2400] [D loss: 0.271885, acc:  36%] [G loss: 5.985241] time: 0:17:44.339640\n",
            "[Epoch 0/5] [Batch 749/2400] [D loss: 0.273709, acc:  35%] [G loss: 5.675091] time: 0:17:45.608050\n",
            "[Epoch 0/5] [Batch 750/2400] [D loss: 0.269990, acc:  45%] [G loss: 5.152919] time: 0:17:46.886833\n",
            "[Epoch 0/5] [Batch 751/2400] [D loss: 0.298707, acc:  48%] [G loss: 7.211267] time: 0:17:48.152895\n",
            "[Epoch 0/5] [Batch 752/2400] [D loss: 0.278285, acc:  40%] [G loss: 5.380939] time: 0:17:49.429931\n",
            "[Epoch 0/5] [Batch 753/2400] [D loss: 0.274682, acc:  34%] [G loss: 5.401079] time: 0:17:50.711936\n",
            "[Epoch 0/5] [Batch 754/2400] [D loss: 0.282389, acc:  42%] [G loss: 7.282467] time: 0:17:51.995703\n",
            "[Epoch 0/5] [Batch 755/2400] [D loss: 0.283185, acc:  46%] [G loss: 7.396698] time: 0:17:53.278103\n",
            "[Epoch 0/5] [Batch 756/2400] [D loss: 0.275201, acc:  50%] [G loss: 6.868438] time: 0:17:54.575894\n",
            "[Epoch 0/5] [Batch 757/2400] [D loss: 0.272811, acc:  44%] [G loss: 6.633770] time: 0:17:55.846366\n",
            "[Epoch 0/5] [Batch 758/2400] [D loss: 0.278912, acc:  32%] [G loss: 6.223595] time: 0:17:57.139898\n",
            "[Epoch 0/5] [Batch 759/2400] [D loss: 0.275699, acc:  47%] [G loss: 5.926236] time: 0:17:58.412911\n",
            "[Epoch 0/5] [Batch 760/2400] [D loss: 0.287378, acc:  52%] [G loss: 5.757106] time: 0:17:59.695088\n",
            "[Epoch 0/5] [Batch 761/2400] [D loss: 0.302546, acc:  47%] [G loss: 7.733830] time: 0:18:00.975946\n",
            "[Epoch 0/5] [Batch 762/2400] [D loss: 0.274345, acc:  46%] [G loss: 5.488928] time: 0:18:02.257230\n",
            "[Epoch 0/5] [Batch 763/2400] [D loss: 0.274034, acc:  45%] [G loss: 6.908299] time: 0:18:03.526433\n",
            "[Epoch 0/5] [Batch 764/2400] [D loss: 0.273752, acc:  48%] [G loss: 5.607646] time: 0:18:04.802169\n",
            "[Epoch 0/5] [Batch 765/2400] [D loss: 0.271903, acc:  38%] [G loss: 5.310948] time: 0:18:06.064517\n",
            "[Epoch 0/5] [Batch 766/2400] [D loss: 0.267325, acc:  47%] [G loss: 5.133011] time: 0:18:07.347713\n",
            "[Epoch 0/5] [Batch 767/2400] [D loss: 0.282677, acc:  47%] [G loss: 6.770486] time: 0:18:08.617994\n",
            "[Epoch 0/5] [Batch 768/2400] [D loss: 0.266228, acc:  51%] [G loss: 5.080978] time: 0:18:09.891347\n",
            "[Epoch 0/5] [Batch 769/2400] [D loss: 0.270711, acc:  42%] [G loss: 4.999223] time: 0:18:11.173874\n",
            "[Epoch 0/5] [Batch 770/2400] [D loss: 0.266202, acc:  48%] [G loss: 4.922685] time: 0:18:12.452676\n",
            "[Epoch 0/5] [Batch 771/2400] [D loss: 0.281020, acc:  50%] [G loss: 7.067693] time: 0:18:13.736988\n",
            "[Epoch 0/5] [Batch 772/2400] [D loss: 0.264400, acc:  47%] [G loss: 4.820705] time: 0:18:15.014519\n",
            "[Epoch 0/5] [Batch 773/2400] [D loss: 0.265217, acc:  47%] [G loss: 4.794945] time: 0:18:16.269158\n",
            "[Epoch 0/5] [Batch 774/2400] [D loss: 0.265695, acc:  51%] [G loss: 4.634768] time: 0:18:17.629366\n",
            "[Epoch 0/5] [Batch 775/2400] [D loss: 0.269230, acc:  47%] [G loss: 5.086170] time: 0:18:18.896951\n",
            "[Epoch 0/5] [Batch 776/2400] [D loss: 0.265921, acc:  49%] [G loss: 4.629803] time: 0:18:20.156336\n",
            "[Epoch 0/5] [Batch 777/2400] [D loss: 0.277430, acc:  53%] [G loss: 7.013437] time: 0:18:21.437102\n",
            "[Epoch 0/5] [Batch 778/2400] [D loss: 0.264368, acc:  50%] [G loss: 4.606183] time: 0:18:22.720162\n",
            "[Epoch 0/5] [Batch 779/2400] [D loss: 0.271211, acc:  37%] [G loss: 5.918094] time: 0:18:24.001423\n",
            "[Epoch 0/5] [Batch 780/2400] [D loss: 0.263332, acc:  48%] [G loss: 4.748315] time: 0:18:25.286862\n",
            "[Epoch 0/5] [Batch 781/2400] [D loss: 0.269007, acc:  51%] [G loss: 8.457252] time: 0:18:26.554920\n",
            "[Epoch 0/5] [Batch 782/2400] [D loss: 0.272670, acc:  34%] [G loss: 5.424018] time: 0:18:27.814267\n",
            "[Epoch 0/5] [Batch 783/2400] [D loss: 0.269672, acc:  46%] [G loss: 6.653854] time: 0:18:29.075462\n",
            "[Epoch 0/5] [Batch 784/2400] [D loss: 0.268202, acc:  36%] [G loss: 5.395667] time: 0:18:30.344463\n",
            "[Epoch 0/5] [Batch 785/2400] [D loss: 0.271021, acc:  41%] [G loss: 5.274376] time: 0:18:31.625902\n",
            "[Epoch 0/5] [Batch 786/2400] [D loss: 0.268271, acc:  46%] [G loss: 5.200208] time: 0:18:32.912003\n",
            "[Epoch 0/5] [Batch 787/2400] [D loss: 0.267444, acc:  55%] [G loss: 4.855892] time: 0:18:34.194464\n",
            "[Epoch 0/5] [Batch 788/2400] [D loss: 0.266942, acc:  57%] [G loss: 5.236396] time: 0:18:35.460279\n",
            "[Epoch 0/5] [Batch 789/2400] [D loss: 0.264809, acc:  50%] [G loss: 5.040072] time: 0:18:36.736294\n",
            "[Epoch 0/5] [Batch 790/2400] [D loss: 0.268799, acc:  50%] [G loss: 5.249761] time: 0:18:38.019344\n",
            "[Epoch 0/5] [Batch 791/2400] [D loss: 0.271638, acc:  53%] [G loss: 7.142266] time: 0:18:39.304104\n",
            "[Epoch 0/5] [Batch 792/2400] [D loss: 0.267005, acc:  36%] [G loss: 4.864419] time: 0:18:40.566121\n",
            "[Epoch 0/5] [Batch 793/2400] [D loss: 0.271218, acc:  48%] [G loss: 7.417730] time: 0:18:41.915111\n",
            "[Epoch 0/5] [Batch 794/2400] [D loss: 0.266263, acc:  46%] [G loss: 5.371136] time: 0:18:43.189438\n",
            "[Epoch 0/5] [Batch 795/2400] [D loss: 0.265792, acc:  42%] [G loss: 5.574422] time: 0:18:44.452881\n",
            "[Epoch 0/5] [Batch 796/2400] [D loss: 0.260797, acc:  48%] [G loss: 5.022669] time: 0:18:45.725229\n",
            "[Epoch 0/5] [Batch 797/2400] [D loss: 0.265119, acc:  53%] [G loss: 5.022206] time: 0:18:46.995453\n",
            "[Epoch 0/5] [Batch 798/2400] [D loss: 0.264831, acc:  51%] [G loss: 7.543645] time: 0:18:48.258688\n",
            "[Epoch 0/5] [Batch 799/2400] [D loss: 0.264486, acc:  39%] [G loss: 5.252176] time: 0:18:49.541217\n",
            "[Epoch 0/5] [Batch 800/2400] [D loss: 0.266529, acc:  48%] [G loss: 8.604793] time: 0:18:50.820987\n",
            "(1, 512, 512, 1)\n",
            "[Epoch 0/5] [Batch 801/2400] [D loss: 0.260893, acc:  48%] [G loss: 6.079741] time: 0:18:56.543072\n",
            "[Epoch 0/5] [Batch 802/2400] [D loss: 0.263949, acc:  45%] [G loss: 8.644706] time: 0:18:57.782465\n",
            "[Epoch 0/5] [Batch 803/2400] [D loss: 0.270253, acc:  38%] [G loss: 6.581384] time: 0:18:59.007202\n",
            "[Epoch 0/5] [Batch 804/2400] [D loss: 0.263979, acc:  46%] [G loss: 5.906372] time: 0:19:00.232628\n",
            "[Epoch 0/5] [Batch 805/2400] [D loss: 0.261771, acc:  49%] [G loss: 5.970697] time: 0:19:01.452308\n",
            "[Epoch 0/5] [Batch 806/2400] [D loss: 0.261251, acc:  52%] [G loss: 5.621465] time: 0:19:02.678837\n",
            "[Epoch 0/5] [Batch 807/2400] [D loss: 0.280889, acc:  59%] [G loss: 5.831355] time: 0:19:03.913017\n",
            "[Epoch 0/5] [Batch 808/2400] [D loss: 0.271123, acc:  57%] [G loss: 5.485703] time: 0:19:05.136631\n",
            "[Epoch 0/5] [Batch 809/2400] [D loss: 0.262993, acc:  40%] [G loss: 9.261468] time: 0:19:06.373983\n",
            "[Epoch 0/5] [Batch 810/2400] [D loss: 0.289951, acc:  42%] [G loss: 6.375737] time: 0:19:07.678084\n",
            "[Epoch 0/5] [Batch 811/2400] [D loss: 0.279271, acc:  42%] [G loss: 5.958871] time: 0:19:08.941089\n",
            "[Epoch 0/5] [Batch 812/2400] [D loss: 0.264268, acc:  42%] [G loss: 5.635580] time: 0:19:10.202299\n",
            "[Epoch 0/5] [Batch 813/2400] [D loss: 0.262451, acc:  44%] [G loss: 5.364970] time: 0:19:11.463473\n",
            "[Epoch 0/5] [Batch 814/2400] [D loss: 0.268923, acc:  44%] [G loss: 7.068838] time: 0:19:12.727155\n",
            "[Epoch 0/5] [Batch 815/2400] [D loss: 0.263883, acc:  36%] [G loss: 5.371811] time: 0:19:14.009460\n",
            "[Epoch 0/5] [Batch 816/2400] [D loss: 0.263819, acc:  42%] [G loss: 5.233809] time: 0:19:15.272787\n",
            "[Epoch 0/5] [Batch 817/2400] [D loss: 0.261810, acc:  42%] [G loss: 5.887658] time: 0:19:16.535465\n",
            "[Epoch 0/5] [Batch 818/2400] [D loss: 0.261524, acc:  40%] [G loss: 5.887107] time: 0:19:17.797053\n",
            "[Epoch 0/5] [Batch 819/2400] [D loss: 0.261328, acc:  38%] [G loss: 5.674385] time: 0:19:19.078937\n",
            "[Epoch 0/5] [Batch 820/2400] [D loss: 0.262424, acc:  39%] [G loss: 5.461428] time: 0:19:20.345841\n",
            "[Epoch 0/5] [Batch 821/2400] [D loss: 0.261476, acc:  42%] [G loss: 5.309757] time: 0:19:21.626904\n",
            "[Epoch 0/5] [Batch 822/2400] [D loss: 0.255399, acc:  49%] [G loss: 7.844865] time: 0:19:22.890227\n",
            "[Epoch 0/5] [Batch 823/2400] [D loss: 0.262397, acc:  39%] [G loss: 5.484700] time: 0:19:24.175880\n",
            "[Epoch 0/5] [Batch 824/2400] [D loss: 0.262677, acc:  38%] [G loss: 5.268247] time: 0:19:25.438788\n",
            "[Epoch 0/5] [Batch 825/2400] [D loss: 0.261326, acc:  40%] [G loss: 5.383397] time: 0:19:26.702046\n",
            "[Epoch 0/5] [Batch 826/2400] [D loss: 0.262224, acc:  37%] [G loss: 5.444807] time: 0:19:27.984280\n",
            "[Epoch 0/5] [Batch 827/2400] [D loss: 0.262444, acc:  39%] [G loss: 5.108009] time: 0:19:29.240234\n",
            "[Epoch 0/5] [Batch 828/2400] [D loss: 0.262266, acc:  37%] [G loss: 5.241731] time: 0:19:30.531481\n",
            "[Epoch 0/5] [Batch 829/2400] [D loss: 0.261195, acc:  40%] [G loss: 4.923283] time: 0:19:31.817541\n",
            "[Epoch 0/5] [Batch 830/2400] [D loss: 0.260969, acc:  47%] [G loss: 5.263348] time: 0:19:33.065953\n",
            "[Epoch 0/5] [Batch 831/2400] [D loss: 0.258809, acc:  40%] [G loss: 5.259886] time: 0:19:34.347977\n",
            "[Epoch 0/5] [Batch 832/2400] [D loss: 0.261571, acc:  38%] [G loss: 5.514094] time: 0:19:35.634032\n",
            "[Epoch 0/5] [Batch 833/2400] [D loss: 0.260517, acc:  39%] [G loss: 5.235342] time: 0:19:36.899376\n",
            "[Epoch 0/5] [Batch 834/2400] [D loss: 0.260208, acc:  40%] [G loss: 5.297031] time: 0:19:38.181619\n",
            "[Epoch 0/5] [Batch 835/2400] [D loss: 0.259715, acc:  43%] [G loss: 4.902537] time: 0:19:39.458201\n",
            "[Epoch 0/5] [Batch 836/2400] [D loss: 0.259842, acc:  46%] [G loss: 5.198557] time: 0:19:40.748222\n",
            "[Epoch 0/5] [Batch 837/2400] [D loss: 0.259053, acc:  39%] [G loss: 5.111712] time: 0:19:42.010201\n",
            "[Epoch 0/5] [Batch 838/2400] [D loss: 0.260730, acc:  39%] [G loss: 4.945845] time: 0:19:43.269590\n",
            "[Epoch 0/5] [Batch 839/2400] [D loss: 0.263518, acc:  39%] [G loss: 5.287990] time: 0:19:44.532656\n",
            "[Epoch 0/5] [Batch 840/2400] [D loss: 0.260170, acc:  41%] [G loss: 4.913640] time: 0:19:45.812812\n",
            "[Epoch 0/5] [Batch 841/2400] [D loss: 0.258444, acc:  44%] [G loss: 5.064365] time: 0:19:47.103234\n",
            "[Epoch 0/5] [Batch 842/2400] [D loss: 0.259312, acc:  44%] [G loss: 4.744108] time: 0:19:48.376676\n",
            "[Epoch 0/5] [Batch 843/2400] [D loss: 0.261233, acc:  40%] [G loss: 5.071528] time: 0:19:49.661699\n",
            "[Epoch 0/5] [Batch 844/2400] [D loss: 0.249230, acc:  47%] [G loss: 8.812374] time: 0:19:50.921366\n",
            "[Epoch 0/5] [Batch 845/2400] [D loss: 0.244439, acc:  52%] [G loss: 7.146079] time: 0:19:52.244700\n",
            "[Epoch 0/5] [Batch 846/2400] [D loss: 0.266283, acc:  44%] [G loss: 5.910655] time: 0:19:53.517198\n",
            "[Epoch 0/5] [Batch 847/2400] [D loss: 0.265290, acc:  38%] [G loss: 5.293184] time: 0:19:54.805808\n",
            "[Epoch 0/5] [Batch 848/2400] [D loss: 0.258672, acc:  51%] [G loss: 5.906587] time: 0:19:56.089312\n",
            "[Epoch 0/5] [Batch 849/2400] [D loss: 0.266666, acc:  57%] [G loss: 5.471474] time: 0:19:57.371721\n",
            "[Epoch 0/5] [Batch 850/2400] [D loss: 0.253594, acc:  56%] [G loss: 8.528221] time: 0:19:58.661476\n",
            "[Epoch 0/5] [Batch 851/2400] [D loss: 0.252564, acc:  46%] [G loss: 5.457239] time: 0:19:59.961033\n",
            "[Epoch 0/5] [Batch 852/2400] [D loss: 0.252696, acc:  44%] [G loss: 5.716753] time: 0:20:01.247559\n",
            "[Epoch 0/5] [Batch 853/2400] [D loss: 0.257458, acc:  44%] [G loss: 4.873562] time: 0:20:02.519302\n",
            "[Epoch 0/5] [Batch 854/2400] [D loss: 0.256522, acc:  43%] [G loss: 5.195553] time: 0:20:03.788559\n",
            "[Epoch 0/5] [Batch 855/2400] [D loss: 0.257036, acc:  52%] [G loss: 4.973543] time: 0:20:05.066752\n",
            "[Epoch 0/5] [Batch 856/2400] [D loss: 0.254924, acc:  47%] [G loss: 5.297447] time: 0:20:06.338883\n",
            "[Epoch 0/5] [Batch 857/2400] [D loss: 0.255922, acc:  44%] [G loss: 4.645037] time: 0:20:07.612182\n",
            "[Epoch 0/5] [Batch 858/2400] [D loss: 0.238996, acc:  60%] [G loss: 6.838648] time: 0:20:08.883084\n",
            "[Epoch 0/5] [Batch 859/2400] [D loss: 0.251892, acc:  46%] [G loss: 5.122172] time: 0:20:10.194771\n",
            "[Epoch 0/5] [Batch 860/2400] [D loss: 0.255082, acc:  42%] [G loss: 5.216527] time: 0:20:11.476344\n",
            "[Epoch 0/5] [Batch 861/2400] [D loss: 0.255006, acc:  42%] [G loss: 4.639977] time: 0:20:12.744514\n",
            "[Epoch 0/5] [Batch 862/2400] [D loss: 0.253448, acc:  45%] [G loss: 4.846451] time: 0:20:14.013960\n",
            "[Epoch 0/5] [Batch 863/2400] [D loss: 0.253010, acc:  43%] [G loss: 4.946869] time: 0:20:15.301851\n",
            "[Epoch 0/5] [Batch 864/2400] [D loss: 0.253630, acc:  42%] [G loss: 4.842354] time: 0:20:16.585658\n",
            "[Epoch 0/5] [Batch 865/2400] [D loss: 0.254775, acc:  42%] [G loss: 5.289513] time: 0:20:17.868290\n",
            "[Epoch 0/5] [Batch 866/2400] [D loss: 0.257472, acc:  56%] [G loss: 4.877311] time: 0:20:19.249804\n",
            "[Epoch 0/5] [Batch 867/2400] [D loss: 0.259087, acc:  49%] [G loss: 5.072309] time: 0:20:20.512325\n",
            "[Epoch 0/5] [Batch 868/2400] [D loss: 0.254105, acc:  47%] [G loss: 6.472010] time: 0:20:21.813594\n",
            "[Epoch 0/5] [Batch 869/2400] [D loss: 0.261118, acc:  38%] [G loss: 4.930460] time: 0:20:23.094585\n",
            "[Epoch 0/5] [Batch 870/2400] [D loss: 0.262520, acc:  39%] [G loss: 4.870787] time: 0:20:24.397605\n",
            "[Epoch 0/5] [Batch 871/2400] [D loss: 0.258775, acc:  54%] [G loss: 7.469762] time: 0:20:25.661272\n",
            "[Epoch 0/5] [Batch 872/2400] [D loss: 0.255255, acc:  55%] [G loss: 5.273368] time: 0:20:26.949428\n",
            "[Epoch 0/5] [Batch 873/2400] [D loss: 0.254092, acc:  43%] [G loss: 5.464565] time: 0:20:28.222171\n",
            "[Epoch 0/5] [Batch 874/2400] [D loss: 0.267797, acc:  41%] [G loss: 4.813595] time: 0:20:29.515761\n",
            "[Epoch 0/5] [Batch 875/2400] [D loss: 0.267452, acc:  41%] [G loss: 5.189140] time: 0:20:30.789790\n",
            "[Epoch 0/5] [Batch 876/2400] [D loss: 0.263705, acc:  38%] [G loss: 4.747737] time: 0:20:32.073665\n",
            "[Epoch 0/5] [Batch 877/2400] [D loss: 0.255203, acc:  45%] [G loss: 6.647173] time: 0:20:33.356939\n",
            "[Epoch 0/5] [Batch 878/2400] [D loss: 0.258669, acc:  42%] [G loss: 4.596333] time: 0:20:34.661938\n",
            "[Epoch 0/5] [Batch 879/2400] [D loss: 0.256956, acc:  49%] [G loss: 4.833009] time: 0:20:35.924051\n",
            "[Epoch 0/5] [Batch 880/2400] [D loss: 0.259163, acc:  50%] [G loss: 4.540856] time: 0:20:37.215438\n",
            "[Epoch 0/5] [Batch 881/2400] [D loss: 0.259488, acc:  41%] [G loss: 4.522941] time: 0:20:38.510592\n",
            "[Epoch 0/5] [Batch 882/2400] [D loss: 0.257577, acc:  40%] [G loss: 4.613659] time: 0:20:39.814762\n",
            "[Epoch 0/5] [Batch 883/2400] [D loss: 0.258202, acc:  53%] [G loss: 4.556580] time: 0:20:41.082304\n",
            "[Epoch 0/5] [Batch 884/2400] [D loss: 0.258645, acc:  56%] [G loss: 5.178926] time: 0:20:42.445733\n",
            "[Epoch 0/5] [Batch 885/2400] [D loss: 0.256440, acc:  53%] [G loss: 5.164935] time: 0:20:43.729213\n",
            "[Epoch 0/5] [Batch 886/2400] [D loss: 0.256771, acc:  50%] [G loss: 5.040236] time: 0:20:45.010857\n",
            "[Epoch 0/5] [Batch 887/2400] [D loss: 0.256751, acc:  51%] [G loss: 4.809167] time: 0:20:46.275880\n",
            "[Epoch 0/5] [Batch 888/2400] [D loss: 0.256860, acc:  47%] [G loss: 4.651014] time: 0:20:47.579923\n",
            "[Epoch 0/5] [Batch 889/2400] [D loss: 0.256133, acc:  40%] [G loss: 4.691717] time: 0:20:48.842778\n",
            "[Epoch 0/5] [Batch 890/2400] [D loss: 0.265317, acc:  41%] [G loss: 7.068848] time: 0:20:50.123682\n",
            "[Epoch 0/5] [Batch 891/2400] [D loss: 0.274894, acc:  43%] [G loss: 4.892168] time: 0:20:51.386342\n",
            "[Epoch 0/5] [Batch 892/2400] [D loss: 0.255988, acc:  52%] [G loss: 7.099041] time: 0:20:52.651539\n",
            "[Epoch 0/5] [Batch 893/2400] [D loss: 0.263536, acc:  59%] [G loss: 4.931242] time: 0:20:53.956623\n",
            "[Epoch 0/5] [Batch 894/2400] [D loss: 0.280395, acc:  58%] [G loss: 4.840237] time: 0:20:55.334555\n",
            "[Epoch 0/5] [Batch 895/2400] [D loss: 0.281004, acc:  56%] [G loss: 6.602219] time: 0:20:56.677912\n",
            "[Epoch 0/5] [Batch 896/2400] [D loss: 0.277403, acc:  56%] [G loss: 4.721501] time: 0:20:58.060345\n",
            "[Epoch 0/5] [Batch 897/2400] [D loss: 0.258650, acc:  48%] [G loss: 5.294232] time: 0:20:59.404640\n",
            "[Epoch 0/5] [Batch 898/2400] [D loss: 0.285772, acc:  42%] [G loss: 5.038959] time: 0:21:00.774903\n",
            "[Epoch 0/5] [Batch 899/2400] [D loss: 0.317044, acc:  43%] [G loss: 4.988036] time: 0:21:02.107107\n",
            "[Epoch 0/5] [Batch 900/2400] [D loss: 0.308456, acc:  40%] [G loss: 6.355362] time: 0:21:03.449054\n",
            "[Epoch 0/5] [Batch 901/2400] [D loss: 0.255626, acc:  46%] [G loss: 5.510509] time: 0:21:04.711299\n",
            "[Epoch 0/5] [Batch 902/2400] [D loss: 0.241353, acc:  51%] [G loss: 5.614013] time: 0:21:05.997674\n",
            "[Epoch 0/5] [Batch 903/2400] [D loss: 0.253972, acc:  50%] [G loss: 5.472317] time: 0:21:07.276770\n",
            "[Epoch 0/5] [Batch 904/2400] [D loss: 0.287414, acc:  49%] [G loss: 5.289360] time: 0:21:08.546285\n",
            "[Epoch 0/5] [Batch 905/2400] [D loss: 0.280150, acc:  45%] [G loss: 5.382230] time: 0:21:09.800862\n",
            "[Epoch 0/5] [Batch 906/2400] [D loss: 0.241685, acc:  58%] [G loss: 7.294057] time: 0:21:11.098456\n",
            "[Epoch 0/5] [Batch 907/2400] [D loss: 0.301553, acc:  59%] [G loss: 5.174226] time: 0:21:12.361905\n",
            "[Epoch 0/5] [Batch 908/2400] [D loss: 0.349355, acc:  58%] [G loss: 5.124331] time: 0:21:13.623900\n",
            "[Epoch 0/5] [Batch 909/2400] [D loss: 0.304122, acc:  59%] [G loss: 5.063656] time: 0:21:14.895973\n",
            "[Epoch 0/5] [Batch 910/2400] [D loss: 0.265216, acc:  54%] [G loss: 4.826516] time: 0:21:16.189079\n",
            "[Epoch 0/5] [Batch 911/2400] [D loss: 0.258829, acc:  44%] [G loss: 4.735253] time: 0:21:17.468598\n",
            "[Epoch 0/5] [Batch 912/2400] [D loss: 0.256213, acc:  39%] [G loss: 5.320515] time: 0:21:18.750907\n",
            "[Epoch 0/5] [Batch 913/2400] [D loss: 0.260006, acc:  37%] [G loss: 4.638868] time: 0:21:20.024171\n",
            "[Epoch 0/5] [Batch 914/2400] [D loss: 0.261625, acc:  38%] [G loss: 5.323977] time: 0:21:21.294618\n",
            "[Epoch 0/5] [Batch 915/2400] [D loss: 0.265342, acc:  35%] [G loss: 5.218167] time: 0:21:22.559099\n",
            "[Epoch 0/5] [Batch 916/2400] [D loss: 0.257708, acc:  39%] [G loss: 4.612971] time: 0:21:23.836670\n",
            "[Epoch 0/5] [Batch 917/2400] [D loss: 0.249430, acc:  49%] [G loss: 7.161766] time: 0:21:25.119493\n",
            "[Epoch 0/5] [Batch 918/2400] [D loss: 0.253036, acc:  50%] [G loss: 5.187814] time: 0:21:26.379079\n",
            "[Epoch 0/5] [Batch 919/2400] [D loss: 0.260445, acc:  43%] [G loss: 5.001095] time: 0:21:27.624299\n",
            "[Epoch 0/5] [Batch 920/2400] [D loss: 0.262481, acc:  36%] [G loss: 4.930211] time: 0:21:28.902457\n",
            "[Epoch 0/5] [Batch 921/2400] [D loss: 0.264677, acc:  37%] [G loss: 6.397010] time: 0:21:30.191142\n",
            "[Epoch 0/5] [Batch 922/2400] [D loss: 0.260360, acc:  43%] [G loss: 4.675353] time: 0:21:31.489724\n",
            "[Epoch 0/5] [Batch 923/2400] [D loss: 0.260584, acc:  57%] [G loss: 4.562902] time: 0:21:32.755678\n",
            "[Epoch 0/5] [Batch 924/2400] [D loss: 0.264864, acc:  49%] [G loss: 5.097069] time: 0:21:34.037885\n",
            "[Epoch 0/5] [Batch 925/2400] [D loss: 0.253652, acc:  46%] [G loss: 7.365391] time: 0:21:35.319336\n",
            "[Epoch 0/5] [Batch 926/2400] [D loss: 0.258822, acc:  40%] [G loss: 5.015220] time: 0:21:36.584919\n",
            "[Epoch 0/5] [Batch 927/2400] [D loss: 0.261193, acc:  37%] [G loss: 5.067049] time: 0:21:37.846773\n",
            "[Epoch 0/5] [Batch 928/2400] [D loss: 0.259826, acc:  38%] [G loss: 5.119891] time: 0:21:39.110843\n",
            "[Epoch 0/5] [Batch 929/2400] [D loss: 0.263558, acc:  35%] [G loss: 4.764709] time: 0:21:40.415485\n",
            "[Epoch 0/5] [Batch 930/2400] [D loss: 0.261397, acc:  42%] [G loss: 6.406803] time: 0:21:41.690373\n",
            "[Epoch 0/5] [Batch 931/2400] [D loss: 0.263011, acc:  37%] [G loss: 4.916315] time: 0:21:43.021680\n",
            "[Epoch 0/5] [Batch 932/2400] [D loss: 0.263945, acc:  34%] [G loss: 4.849922] time: 0:21:44.344678\n",
            "[Epoch 0/5] [Batch 933/2400] [D loss: 0.258160, acc:  44%] [G loss: 4.647396] time: 0:21:45.687296\n",
            "[Epoch 0/5] [Batch 934/2400] [D loss: 0.261558, acc:  45%] [G loss: 4.886073] time: 0:21:46.992575\n",
            "[Epoch 0/5] [Batch 935/2400] [D loss: 0.262028, acc:  38%] [G loss: 4.605576] time: 0:21:48.273784\n",
            "[Epoch 0/5] [Batch 936/2400] [D loss: 0.259488, acc:  42%] [G loss: 4.844003] time: 0:21:49.558915\n",
            "[Epoch 0/5] [Batch 937/2400] [D loss: 0.261500, acc:  44%] [G loss: 4.718616] time: 0:21:50.823652\n",
            "[Epoch 0/5] [Batch 938/2400] [D loss: 0.258196, acc:  58%] [G loss: 4.834574] time: 0:21:52.105565\n",
            "[Epoch 0/5] [Batch 939/2400] [D loss: 0.264982, acc:  58%] [G loss: 4.582497] time: 0:21:53.370455\n",
            "[Epoch 0/5] [Batch 940/2400] [D loss: 0.261114, acc:  58%] [G loss: 4.541168] time: 0:21:54.752696\n",
            "[Epoch 0/5] [Batch 941/2400] [D loss: 0.263051, acc:  49%] [G loss: 6.668012] time: 0:21:56.057153\n",
            "[Epoch 0/5] [Batch 942/2400] [D loss: 0.257355, acc:  56%] [G loss: 5.018638] time: 0:21:57.339408\n",
            "[Epoch 0/5] [Batch 943/2400] [D loss: 0.263769, acc:  51%] [G loss: 6.394679] time: 0:21:58.609658\n",
            "[Epoch 0/5] [Batch 944/2400] [D loss: 0.259979, acc:  61%] [G loss: 4.842945] time: 0:21:59.885378\n",
            "[Epoch 0/5] [Batch 945/2400] [D loss: 0.258643, acc:  57%] [G loss: 6.125041] time: 0:22:01.150676\n",
            "[Epoch 0/5] [Batch 946/2400] [D loss: 0.259185, acc:  50%] [G loss: 4.925240] time: 0:22:02.452558\n",
            "[Epoch 0/5] [Batch 947/2400] [D loss: 0.253800, acc:  54%] [G loss: 4.924290] time: 0:22:03.730899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b3676ca09588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mADDN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trail'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5984b48ca030>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# Train the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}